# kubeadm-highavailiability (English / ä¸­æ–‡) - åŸºäºkubeadmçš„kubernetesé«˜å¯ç”¨é›†ç¾¤éƒ¨ç½²ï¼Œæ”¯æŒv1.11.x v1.9.x v1.7.x v1.6.xç‰ˆæœ¬

![k8s logo](images/Kubernetes.png)

- [ä¸­æ–‡æ–‡æ¡£(for v1.11.xç‰ˆæœ¬)](README_CN.md)
- [English document(for v1.11.x version)](README.md)
- [ä¸­æ–‡æ–‡æ¡£(for v1.9.xç‰ˆæœ¬)](v1.9/README_CN.md)
- [English document(for v1.9.x version)](v1.9/README.md)
- [ä¸­æ–‡æ–‡æ¡£(for v1.7.xç‰ˆæœ¬)](v1.7/README_CN.md)
- [English document(for v1.7.x version)](v1.7/README.md)
- [ä¸­æ–‡æ–‡æ¡£(for v1.6.xç‰ˆæœ¬)](v1.6/README_CN.md)
- [English document(for v1.6.x version)](v1.6/README.md)

---

- [GitHubé¡¹ç›®åœ°å€](https://github.com/cookeem/kubeadm-ha/)
- [OSChinaé¡¹ç›®åœ°å€](https://git.oschina.net/cookeem/kubeadm-ha/)

---

- è¯¥æŒ‡å¼•é€‚ç”¨äºv1.11.xç‰ˆæœ¬çš„kubernetesé›†ç¾¤

> v1.11.xç‰ˆæœ¬æ”¯æŒåœ¨control planeä¸Šå¯åŠ¨TLSçš„etcdé«˜å¯ç”¨é›†ç¾¤ã€‚

### ç›®å½•

1. [éƒ¨ç½²æ¶æ„](#éƒ¨ç½²æ¶æ„)
    1. [æ¦‚è¦éƒ¨ç½²æ¶æ„](#æ¦‚è¦éƒ¨ç½²æ¶æ„)
    1. [è¯¦ç»†éƒ¨ç½²æ¶æ„](#è¯¦ç»†éƒ¨ç½²æ¶æ„)
    1. [ä¸»æœºèŠ‚ç‚¹æ¸…å•](#ä¸»æœºèŠ‚ç‚¹æ¸…å•)
1. [å®‰è£…å‰å‡†å¤‡](#å®‰è£…å‰å‡†å¤‡)
    1. [ç‰ˆæœ¬ä¿¡æ¯](#ç‰ˆæœ¬ä¿¡æ¯)
    1. [æ‰€éœ€dockeré•œåƒ](#æ‰€éœ€dockeré•œåƒ)
    1. [ç³»ç»Ÿè®¾ç½®](#ç³»ç»Ÿè®¾ç½®)
1. [kuberneteså®‰è£…](#kuberneteså®‰è£…)
    1. [firewalldå’Œiptablesç›¸å…³ç«¯å£è®¾ç½®](#firewalldå’Œiptablesç›¸å…³ç«¯å£è®¾ç½®)
    1. [kubernetesç›¸å…³æœåŠ¡å®‰è£…](#kubernetesç›¸å…³æœåŠ¡å®‰è£…)
    1. [masterèŠ‚ç‚¹äº’ä¿¡è®¾ç½®](#masterèŠ‚ç‚¹äº’ä¿¡è®¾ç½®)
1. [masteré«˜å¯ç”¨å®‰è£…](#masteré«˜å¯ç”¨å®‰è£…)
    1. [é…ç½®æ–‡ä»¶åˆå§‹åŒ–](#é…ç½®æ–‡ä»¶åˆå§‹åŒ–)
    1. [kubeadmåˆå§‹åŒ–](#kubeadmåˆå§‹åŒ–)
    1. [é«˜å¯ç”¨é…ç½®](#é«˜å¯ç”¨é…ç½®)
1. [masterè´Ÿè½½å‡è¡¡è®¾ç½®](#masterè´Ÿè½½å‡è¡¡è®¾ç½®)
    1. [keepalivedå®‰è£…é…ç½®](#keepalivedå®‰è£…é…ç½®)
    1. [nginxè´Ÿè½½å‡è¡¡é…ç½®](#nginxè´Ÿè½½å‡è¡¡é…ç½®)
    1. [kube-proxyé«˜å¯ç”¨è®¾ç½®](#kube-proxyé«˜å¯ç”¨è®¾ç½®)
    1. [éªŒè¯é«˜å¯ç”¨çŠ¶æ€](#éªŒè¯é«˜å¯ç”¨çŠ¶æ€)
    1. [åŸºç¡€ç»„ä»¶å®‰è£…](#åŸºç¡€ç»„ä»¶å®‰è£…)
1. [workerèŠ‚ç‚¹è®¾ç½®](#workerèŠ‚ç‚¹è®¾ç½®)
    1. [workeråŠ å…¥é«˜å¯ç”¨é›†ç¾¤](#workeråŠ å…¥é«˜å¯ç”¨é›†ç¾¤)
1. [é›†ç¾¤éªŒè¯](#é›†ç¾¤éªŒè¯)
    1. [éªŒè¯é›†ç¾¤é«˜å¯ç”¨è®¾ç½®](#éªŒè¯é›†ç¾¤é«˜å¯ç”¨è®¾ç½®)
1. [é›†ç¾¤å‡çº§](#é›†ç¾¤å‡çº§)
    1. [é›†ç¾¤å‡çº§v1.11.1å‡çº§v1.11.5](#é›†ç¾¤å‡çº§v1-11-1å‡çº§v1-11-5)

### éƒ¨ç½²æ¶æ„

#### æ¦‚è¦éƒ¨ç½²æ¶æ„

![ha logo](images/ha.png)

- kubernetesé«˜å¯ç”¨çš„æ ¸å¿ƒæ¶æ„æ˜¯masterçš„é«˜å¯ç”¨ï¼Œkubectlã€å®¢æˆ·ç«¯ä»¥åŠnodesè®¿é—®load balancerå®ç°é«˜å¯ç”¨ã€‚

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### è¯¦ç»†éƒ¨ç½²æ¶æ„

![k8s ha](images/k8s-ha.png)

- kubernetesç»„ä»¶è¯´æ˜

> kube-apiserverï¼šé›†ç¾¤æ ¸å¿ƒï¼Œé›†ç¾¤APIæ¥å£ã€é›†ç¾¤å„ä¸ªç»„ä»¶é€šä¿¡çš„ä¸­æ¢ï¼›é›†ç¾¤å®‰å…¨æ§åˆ¶ï¼›

> etcdï¼šé›†ç¾¤çš„æ•°æ®ä¸­å¿ƒï¼Œç”¨äºå­˜æ”¾é›†ç¾¤çš„é…ç½®ä»¥åŠçŠ¶æ€ä¿¡æ¯ï¼Œéå¸¸é‡è¦ï¼Œå¦‚æœæ•°æ®ä¸¢å¤±é‚£ä¹ˆé›†ç¾¤å°†æ— æ³•æ¢å¤ï¼›å› æ­¤é«˜å¯ç”¨é›†ç¾¤éƒ¨ç½²é¦–å…ˆå°±æ˜¯etcdæ˜¯é«˜å¯ç”¨é›†ç¾¤ï¼›

> kube-schedulerï¼šé›†ç¾¤Podçš„è°ƒåº¦ä¸­å¿ƒï¼›é»˜è®¤kubeadmå®‰è£…æƒ…å†µä¸‹--leader-electå‚æ•°å·²ç»è®¾ç½®ä¸ºtrueï¼Œä¿è¯masteré›†ç¾¤ä¸­åªæœ‰ä¸€ä¸ªkube-schedulerå¤„äºæ´»è·ƒçŠ¶æ€ï¼›

> kube-controller-managerï¼šé›†ç¾¤çŠ¶æ€ç®¡ç†å™¨ï¼Œå½“é›†ç¾¤çŠ¶æ€ä¸æœŸæœ›ä¸åŒæ—¶ï¼Œkcmä¼šåŠªåŠ›è®©é›†ç¾¤æ¢å¤æœŸæœ›çŠ¶æ€ï¼Œæ¯”å¦‚ï¼šå½“ä¸€ä¸ªpodæ­»æ‰ï¼Œkcmä¼šåŠªåŠ›æ–°å»ºä¸€ä¸ªpodæ¥æ¢å¤å¯¹åº”replicas setæœŸæœ›çš„çŠ¶æ€ï¼›é»˜è®¤kubeadmå®‰è£…æƒ…å†µä¸‹--leader-electå‚æ•°å·²ç»è®¾ç½®ä¸ºtrueï¼Œä¿è¯masteré›†ç¾¤ä¸­åªæœ‰ä¸€ä¸ªkube-controller-managerå¤„äºæ´»è·ƒçŠ¶æ€ï¼›

> kubelet: kubernetes node agentï¼Œè´Ÿè´£ä¸nodeä¸Šçš„docker engineæ‰“äº¤é“ï¼›

> kube-proxy: æ¯ä¸ªnodeä¸Šä¸€ä¸ªï¼Œè´Ÿè´£service vipåˆ°endpoint podçš„æµé‡è½¬å‘ï¼Œå½“å‰ä¸»è¦é€šè¿‡è®¾ç½®iptablesè§„åˆ™å®ç°ã€‚

- è´Ÿè½½å‡è¡¡

> keepalivedé›†ç¾¤è®¾ç½®ä¸€ä¸ªè™šæ‹Ÿipåœ°å€ï¼Œè™šæ‹Ÿipåœ°å€æŒ‡å‘k8s-master01ã€k8s-master02ã€k8s-master03ã€‚

> nginxç”¨äºk8s-master01ã€k8s-master02ã€k8s-master03çš„apiserverçš„è´Ÿè½½å‡è¡¡ã€‚å¤–éƒ¨kubectlä»¥åŠnodesè®¿é—®apiserverçš„æ—¶å€™å°±å¯ä»¥ç”¨è¿‡keepalivedçš„è™šæ‹Ÿip(192.168.20.10)ä»¥åŠnginxç«¯å£(16443)è®¿é—®masteré›†ç¾¤çš„apiserverã€‚

---

[è¿”å›ç›®å½•](#ç›®å½•)

#### ä¸»æœºèŠ‚ç‚¹æ¸…å•

ä¸»æœºå | IPåœ°å€ | è¯´æ˜ | ç»„ä»¶
:--- | :--- | :--- | :---
k8s-master01 ~ 03 | 192.168.20.20 ~ 22 | masterèŠ‚ç‚¹ * 3 | keepalivedã€nginxã€etcdã€kubeletã€kube-apiserver
k8s-master-lb     | 192.168.20.10 | keepalivedè™šæ‹ŸIP | æ— 
k8s-node01 ~ 08   | 192.168.20.30 ~ 37 | workerèŠ‚ç‚¹ * 8 | kubelet

---

[è¿”å›ç›®å½•](#ç›®å½•)

### å®‰è£…å‰å‡†å¤‡

#### ç‰ˆæœ¬ä¿¡æ¯

- Linuxç‰ˆæœ¬ï¼šCentOS 7.4.1708

- å†…æ ¸ç‰ˆæœ¬: 4.6.4-1.el7.elrepo.x86_64

```sh
$ cat /etc/redhat-release
CentOS Linux release 7.4.1708 (Core)

$ uname -r
4.6.4-1.el7.elrepo.x86_64
```

- dockerç‰ˆæœ¬ï¼š17.12.0-ce-rc2

```sh
$ docker version
Client:
 Version:	17.12.0-ce-rc2
 API version:	1.35
 Go version:	go1.9.2
 Git commit:	f9cde63
 Built:	Tue Dec 12 06:42:20 2017
 OS/Arch:	linux/amd64

Server:
 Engine:
  Version:	17.12.0-ce-rc2
  API version:	1.35 (minimum version 1.12)
  Go version:	go1.9.2
  Git commit:	f9cde63
  Built:	Tue Dec 12 06:44:50 2017
  OS/Arch:	linux/amd64
  Experimental:	false
```

- kubeadmç‰ˆæœ¬ï¼šv1.11.1

```sh
$ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"11", GitVersion:"v1.11.1", GitCommit:"b1b29978270dc22fecc592ac55d903350454310a", GitTreeState:"clean", BuildDate:"2018-07-17T18:50:16Z", GoVersion:"go1.10.3", Compiler:"gc", Platform:"linux/amd64"}
```

- kubeletç‰ˆæœ¬ï¼šv1.11.1

```sh
$ kubelet --version
Kubernetes v1.11.1
```

- ç½‘ç»œç»„ä»¶

> calico

---

[è¿”å›ç›®å½•](#ç›®å½•)

#### æ‰€éœ€dockeré•œåƒ

- ç›¸å…³dockeré•œåƒä»¥åŠç‰ˆæœ¬

```sh
# kuberentes basic components

# é€šè¿‡kubeadm è·å–åŸºç¡€ç»„ä»¶é•œåƒæ¸…å•
$ kubeadm config images list --kubernetes-version=v1.11.1
k8s.gcr.io/kube-apiserver-amd64:v1.11.1
k8s.gcr.io/kube-controller-manager-amd64:v1.11.1
k8s.gcr.io/kube-scheduler-amd64:v1.11.1
k8s.gcr.io/kube-proxy-amd64:v1.11.1
k8s.gcr.io/pause:3.1
k8s.gcr.io/etcd-amd64:3.2.18
k8s.gcr.io/coredns:1.1.3

# é€šè¿‡kubeadm æ‹‰å–åŸºç¡€é•œåƒ
$ kubeadm config images pull --kubernetes-version=v1.11.1

# kubernetes networks add ons
$ docker pull quay.io/calico/typha:v0.7.4
$ docker pull quay.io/calico/node:v3.1.3
$ docker pull quay.io/calico/cni:v3.1.3

# kubernetes metrics server
$ docker pull gcr.io/google_containers/metrics-server-amd64:v0.2.1

# kubernetes dashboard
$ docker pull gcr.io/google_containers/kubernetes-dashboard-amd64:v1.8.3

# kubernetes heapster
$ docker pull k8s.gcr.io/heapster-amd64:v1.5.4
$ docker pull k8s.gcr.io/heapster-influxdb-amd64:v1.5.2
$ docker pull k8s.gcr.io/heapster-grafana-amd64:v5.0.4

# kubernetes apiserver load balancer
$ docker pull nginx:latest

# prometheus
$ docker pull prom/prometheus:v2.3.1

# traefik
$ docker pull traefik:v1.6.3

# istio
$ docker pull docker.io/jaegertracing/all-in-one:1.5
$ docker pull docker.io/prom/prometheus:v2.3.1
$ docker pull docker.io/prom/statsd-exporter:v0.6.0
$ docker pull gcr.io/istio-release/citadel:1.0.0
$ docker pull gcr.io/istio-release/galley:1.0.0
$ docker pull gcr.io/istio-release/grafana:1.0.0
$ docker pull gcr.io/istio-release/mixer:1.0.0
$ docker pull gcr.io/istio-release/pilot:1.0.0
$ docker pull gcr.io/istio-release/proxy_init:1.0.0
$ docker pull gcr.io/istio-release/proxyv2:1.0.0
$ docker pull gcr.io/istio-release/servicegraph:1.0.0
$ docker pull gcr.io/istio-release/sidecar_injector:1.0.0
$ docker pull quay.io/coreos/hyperkube:v1.7.6_coreos.0
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

#### ç³»ç»Ÿè®¾ç½®

- åœ¨æ‰€æœ‰kubernetesèŠ‚ç‚¹ä¸Šå¢åŠ kubernetesä»“åº“

```sh
$ cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kube*
EOF
```

- åœ¨æ‰€æœ‰kubernetesèŠ‚ç‚¹ä¸Šè¿›è¡Œç³»ç»Ÿæ›´æ–°

```sh
$ yum update -y
```

- åœ¨æ‰€æœ‰kubernetesèŠ‚ç‚¹ä¸Šè®¾ç½®SELINUXä¸ºpermissiveæ¨¡å¼

```sh
$ vi /etc/selinux/config
SELINUX=permissive

$ setenforce 0
```

- åœ¨æ‰€æœ‰kubernetesèŠ‚ç‚¹ä¸Šè®¾ç½®iptableså‚æ•°

```sh
$ cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

$ sysctl --system
```

- åœ¨æ‰€æœ‰kubernetesèŠ‚ç‚¹ä¸Šç¦ç”¨swap

```sh
$ swapoff -a

# ç¦ç”¨fstabä¸­çš„swapé¡¹ç›®
$ vi /etc/fstab
#/dev/mapper/centos-swap swap                    swap    defaults        0 0

# ç¡®è®¤swapå·²ç»è¢«ç¦ç”¨
$ cat /proc/swaps
Filename                Type        Size    Used    Priority
```

- åœ¨æ‰€æœ‰kubernetesèŠ‚ç‚¹ä¸Šé‡å¯ä¸»æœº

```sh
# é‡å¯ä¸»æœº
$ reboot
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

### kuberneteså®‰è£…

#### firewalldå’Œiptablesç›¸å…³ç«¯å£è®¾ç½®

- æ‰€æœ‰èŠ‚ç‚¹å¼€å¯é˜²ç«å¢™

```sh
# é‡å¯é˜²ç«å¢™
$ systemctl enable firewalld
$ systemctl restart firewalld
$ systemctl status firewalld
```

- ç›¸å…³ç«¯å£ï¼ˆmasterï¼‰

åè®® | æ–¹å‘ | ç«¯å£ | è¯´æ˜
:--- | :--- | :--- | :---
TCP | Inbound | 16443*    | Load balancer Kubernetes API server port
TCP | Inbound | 6443*     | Kubernetes API server
TCP | Inbound | 4001      | etcd listen client port
TCP | Inbound | 2379-2380 | etcd server client API
TCP | Inbound | 10250     | Kubelet API
TCP | Inbound | 10251     | kube-scheduler
TCP | Inbound | 10252     | kube-controller-manager
TCP | Inbound | 10255     | Read-only Kubelet API (Deprecated)
TCP | Inbound | 30000-32767 | NodePort Services

- è®¾ç½®é˜²ç«å¢™ç­–ç•¥

```sh
$ firewall-cmd --zone=public --add-port=16443/tcp --permanent
$ firewall-cmd --zone=public --add-port=6443/tcp --permanent
$ firewall-cmd --zone=public --add-port=4001/tcp --permanent
$ firewall-cmd --zone=public --add-port=2379-2380/tcp --permanent
$ firewall-cmd --zone=public --add-port=10250/tcp --permanent
$ firewall-cmd --zone=public --add-port=10251/tcp --permanent
$ firewall-cmd --zone=public --add-port=10252/tcp --permanent
$ firewall-cmd --zone=public --add-port=30000-32767/tcp --permanent

$ firewall-cmd --reload

$ firewall-cmd --list-all --zone=public
public (active)
  target: default
  icmp-block-inversion: no
  interfaces: ens2f1 ens1f0 nm-bond
  sources:
  services: ssh dhcpv6-client
  ports: 4001/tcp 6443/tcp 2379-2380/tcp 10250/tcp 10251/tcp 10252/tcp 30000-32767/tcp
  protocols:
  masquerade: no
  forward-ports:
  source-ports:
  icmp-blocks:
  rich rules:
```

- ç›¸å…³ç«¯å£ï¼ˆworkerï¼‰

åè®® | æ–¹å‘ | ç«¯å£ | è¯´æ˜
:--- | :--- | :--- | :---
TCP | Inbound | 10250       | Kubelet API
TCP | Inbound | 30000-32767 | NodePort Services

- è®¾ç½®é˜²ç«å¢™ç­–ç•¥

```sh
$ firewall-cmd --zone=public --add-port=10250/tcp --permanent
$ firewall-cmd --zone=public --add-port=30000-32767/tcp --permanent

$ firewall-cmd --reload

$ firewall-cmd --list-all --zone=public
public (active)
  target: default
  icmp-block-inversion: no
  interfaces: ens2f1 ens1f0 nm-bond
  sources:
  services: ssh dhcpv6-client
  ports: 10250/tcp 30000-32767/tcp
  protocols:
  masquerade: no
  forward-ports:
  source-ports:
  icmp-blocks:
  rich rules:
```

- åœ¨æ‰€æœ‰kubernetesèŠ‚ç‚¹ä¸Šå…è®¸kube-proxyçš„forward

```sh
$ firewall-cmd --permanent --direct --add-rule ipv4 filter INPUT 1 -i docker0 -j ACCEPT -m comment --comment "kube-proxy redirects"
$ firewall-cmd --permanent --direct --add-rule ipv4 filter FORWARD 1 -o docker0 -j ACCEPT -m comment --comment "docker subnet"
$ firewall-cmd --reload

$ firewall-cmd --direct --get-all-rules
ipv4 filter INPUT 1 -i docker0 -j ACCEPT -m comment --comment 'kube-proxy redirects'
ipv4 filter FORWARD 1 -o docker0 -j ACCEPT -m comment --comment 'docker subnet'

# é‡å¯é˜²ç«å¢™
$ systemctl restart firewalld
```

- è§£å†³kube-proxyæ— æ³•å¯ç”¨nodePortï¼Œé‡å¯firewalldå¿…é¡»æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œåœ¨æ‰€æœ‰èŠ‚ç‚¹è®¾ç½®å®šæ—¶ä»»åŠ¡

```sh
$ crontab -e
0,5,10,15,20,25,30,35,40,45,50,55 * * * * /usr/sbin/iptables -D INPUT -j REJECT --reject-with icmp-host-prohibited
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

#### kubernetesç›¸å…³æœåŠ¡å®‰è£…

- åœ¨æ‰€æœ‰kubernetesèŠ‚ç‚¹ä¸Šå®‰è£…å¹¶å¯åŠ¨kubernetes

```sh
$ yum install -y docker-ce-17.12.0.ce-0.2.rc2.el7.centos.x86_64
$ yum install -y docker-compose-1.9.0-5.el7.noarch
$ systemctl enable docker && systemctl start docker

$ yum install -y kubelet-1.11.1-0.x86_64 kubeadm-1.11.1-0.x86_64 kubectl-1.11.1-0.x86_64
$ systemctl enable kubelet && systemctl start kubelet
```

- åœ¨æ‰€æœ‰masterèŠ‚ç‚¹å®‰è£…å¹¶å¯åŠ¨keepalived

```sh
$ yum install -y keepalived
$ systemctl enable keepalived && systemctl restart keepalived
```

#### masterèŠ‚ç‚¹äº’ä¿¡è®¾ç½®

- åœ¨k8s-master01èŠ‚ç‚¹ä¸Šè®¾ç½®èŠ‚ç‚¹äº’ä¿¡

```sh
$ rm -rf /root/.ssh/*
$ ssh k8s-master01 pwd
$ ssh k8s-master02 rm -rf /root/.ssh/*
$ ssh k8s-master03 rm -rf /root/.ssh/*
$ ssh k8s-master02 mkdir -p /root/.ssh/
$ ssh k8s-master03 mkdir -p /root/.ssh/

$ scp /root/.ssh/known_hosts root@k8s-master02:/root/.ssh/
$ scp /root/.ssh/known_hosts root@k8s-master03:/root/.ssh/

$ ssh-keygen -t rsa -P '' -f /root/.ssh/id_rsa
$ cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
$ scp /root/.ssh/authorized_keys root@k8s-master02:/root/.ssh/
```

- åœ¨k8s-master02èŠ‚ç‚¹ä¸Šè®¾ç½®èŠ‚ç‚¹äº’ä¿¡

```sh
$ ssh-keygen -t rsa -P '' -f /root/.ssh/id_rsa
$ cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
$ scp /root/.ssh/authorized_keys root@k8s-master03:/root/.ssh/
```

- åœ¨k8s-master03èŠ‚ç‚¹ä¸Šè®¾ç½®èŠ‚ç‚¹äº’ä¿¡

```sh
$ ssh-keygen -t rsa -P '' -f /root/.ssh/id_rsa
$ cat /root/.ssh/id_rsa.pub >> /root/.ssh/authorized_keys
$ scp /root/.ssh/authorized_keys root@k8s-master01:/root/.ssh/
$ scp /root/.ssh/authorized_keys root@k8s-master02:/root/.ssh/
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

### masteré«˜å¯ç”¨å®‰è£…

#### é…ç½®æ–‡ä»¶åˆå§‹åŒ–

- åœ¨k8s-master01ä¸Šå…‹éš†kubeadm-haé¡¹ç›®æºç 

```sh
$ git clone https://github.com/cookeem/kubeadm-ha
```

- åœ¨k8s-master01ä¸Šé€šè¿‡`create-config.sh`è„šæœ¬åˆ›å»ºç›¸å…³é…ç½®æ–‡ä»¶

```sh
$ cd kubeadm-ha

# æ ¹æ®create-config.shçš„æç¤ºï¼Œä¿®æ”¹ä»¥ä¸‹é…ç½®ä¿¡æ¯
$ vi create-config.sh
# master keepalived virtual ip address
export K8SHA_VIP=192.168.60.79
# master01 ip address
export K8SHA_IP1=192.168.60.72
# master02 ip address
export K8SHA_IP2=192.168.60.77
# master03 ip address
export K8SHA_IP3=192.168.60.78
# master keepalived virtual ip hostname
export K8SHA_VHOST=k8s-master-lb
# master01 hostname
export K8SHA_HOST1=k8s-master01
# master02 hostname
export K8SHA_HOST2=k8s-master02
# master03 hostname
export K8SHA_HOST3=k8s-master03
# master01 network interface name
export K8SHA_NETINF1=nm-bond
# master02 network interface name
export K8SHA_NETINF2=nm-bond
# master03 network interface name
export K8SHA_NETINF3=nm-bond
# keepalived auth_pass config
export K8SHA_KEEPALIVED_AUTH=412f7dc3bfed32194d1600c483e10ad1d
# calico reachable ip address
export K8SHA_CALICO_REACHABLE_IP=192.168.60.1
# kubernetes CIDR pod subnet, if CIDR pod subnet is "172.168.0.0/16" please set to "172.168.0.0"
export K8SHA_CIDR=172.168.0.0

# ä»¥ä¸‹è„šæœ¬ä¼šåˆ›å»º3ä¸ªmasterèŠ‚ç‚¹çš„kubeadmé…ç½®æ–‡ä»¶ï¼Œkeepalivedé…ç½®æ–‡ä»¶ï¼Œnginxè´Ÿè½½å‡è¡¡é…ç½®æ–‡ä»¶ï¼Œä»¥åŠcalicoé…ç½®æ–‡ä»¶
$ ./create-config.sh
create kubeadm-config.yaml files success. config/k8s-master01/kubeadm-config.yaml
create kubeadm-config.yaml files success. config/k8s-master02/kubeadm-config.yaml
create kubeadm-config.yaml files success. config/k8s-master03/kubeadm-config.yaml
create keepalived files success. config/k8s-master01/keepalived/
create keepalived files success. config/k8s-master02/keepalived/
create keepalived files success. config/k8s-master03/keepalived/
create nginx-lb files success. config/k8s-master01/nginx-lb/
create nginx-lb files success. config/k8s-master02/nginx-lb/
create nginx-lb files success. config/k8s-master03/nginx-lb/
create calico.yaml file success. calico/calico.yaml

# è®¾ç½®ç›¸å…³hostnameå˜é‡
$ export HOST1=k8s-master01
$ export HOST2=k8s-master02
$ export HOST3=k8s-master03

# æŠŠkubeadmé…ç½®æ–‡ä»¶æ”¾åˆ°å„ä¸ªmasterèŠ‚ç‚¹çš„/root/ç›®å½•
$ scp -r config/$HOST1/kubeadm-config.yaml $HOST1:/root/
$ scp -r config/$HOST2/kubeadm-config.yaml $HOST2:/root/
$ scp -r config/$HOST3/kubeadm-config.yaml $HOST3:/root/

# æŠŠkeepalivedé…ç½®æ–‡ä»¶æ”¾åˆ°å„ä¸ªmasterèŠ‚ç‚¹çš„/etc/keepalived/ç›®å½•
$ scp -r config/$HOST1/keepalived/* $HOST1:/etc/keepalived/
$ scp -r config/$HOST2/keepalived/* $HOST2:/etc/keepalived/
$ scp -r config/$HOST3/keepalived/* $HOST3:/etc/keepalived/

# æŠŠnginxè´Ÿè½½å‡è¡¡é…ç½®æ–‡ä»¶æ”¾åˆ°å„ä¸ªmasterèŠ‚ç‚¹çš„/etc/kubernetes/ç›®å½•
$ scp -r config/$HOST1/nginx-lb/nginx-lb.conf $HOST1:/etc/kubernetes/
$ scp -r config/$HOST2/nginx-lb/nginx-lb.conf $HOST2:/etc/kubernetes/
$ scp -r config/$HOST3/nginx-lb/nginx-lb.conf $HOST3:/etc/kubernetes/

# æŠŠnginxè´Ÿè½½å‡è¡¡éƒ¨ç½²æ–‡ä»¶æ”¾åˆ°å„ä¸ªmasterèŠ‚ç‚¹çš„/etc/kubernetes/manifests/ç›®å½•
$ scp -r config/$HOST1/nginx-lb/nginx-lb.yaml $HOST1:/etc/kubernetes/manifests/
$ scp -r config/$HOST2/nginx-lb/nginx-lb.yaml $HOST2:/etc/kubernetes/manifests/
$ scp -r config/$HOST3/nginx-lb/nginx-lb.yaml $HOST3:/etc/kubernetes/manifests/
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

#### kubeadmåˆå§‹åŒ–

- åœ¨k8s-master01èŠ‚ç‚¹ä¸Šä½¿ç”¨kubeadmè¿›è¡Œkubernetesé›†ç¾¤åˆå§‹åŒ–

```sh
# æ‰§è¡Œkubeadm initä¹‹ååŠ¡å¿…è®°å½•æ‰§è¡Œç»“æœè¾“å‡ºçš„${YOUR_TOKEN}ä»¥åŠ${YOUR_DISCOVERY_TOKEN_CA_CERT_HASH}
$ kubeadm init --config /root/kubeadm-config.yaml
kubeadm join 192.168.20.20:6443 --token ${YOUR_TOKEN} --discovery-token-ca-cert-hash sha256:${YOUR_DISCOVERY_TOKEN_CA_CERT_HASH}
```

- åœ¨æ‰€æœ‰masterèŠ‚ç‚¹ä¸Šè®¾ç½®kubectlçš„é…ç½®æ–‡ä»¶å˜é‡

```sh
$ cat <<EOF >> ~/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf
EOF

$ source ~/.bashrc

# éªŒè¯æ˜¯å¦å¯ä»¥ä½¿ç”¨kubectlå®¢æˆ·ç«¯è¿æ¥é›†ç¾¤
$ kubectl get nodes
```

- åœ¨k8s-master01èŠ‚ç‚¹ä¸Šç­‰å¾… etcd / kube-apiserver / kube-controller-manager / kube-scheduler å¯åŠ¨

```sh
$ kubectl get pods -n kube-system -o wide
NAME                                   READY     STATUS    RESTARTS   AGE       IP              NODE
...
etcd-k8s-master01                      1/1       Running   0          18m       192.168.20.20   k8s-master01
kube-apiserver-k8s-master01            1/1       Running   0          18m       192.168.20.20   k8s-master01
kube-controller-manager-k8s-master01   1/1       Running   0          18m       192.168.20.20   k8s-master01
kube-scheduler-k8s-master01            1/1       Running   1          18m       192.168.20.20   k8s-master01
...
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

#### é«˜å¯ç”¨é…ç½®

- åœ¨k8s-master01ä¸ŠæŠŠè¯ä¹¦å¤åˆ¶åˆ°å…¶ä»–master

```sh
# æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ä»¥ä¸‹HOSTNAMESå˜é‡
$ export CONTROL_PLANE_IPS="k8s-master02 k8s-master03"

# æŠŠè¯ä¹¦å¤åˆ¶åˆ°å…¶ä»–masterèŠ‚ç‚¹
$ for host in ${CONTROL_PLANE_IPS}; do
  scp /etc/kubernetes/pki/ca.crt $host:/etc/kubernetes/pki/ca.crt
  scp /etc/kubernetes/pki/ca.key $host:/etc/kubernetes/pki/ca.key
  scp /etc/kubernetes/pki/sa.key $host:/etc/kubernetes/pki/sa.key
  scp /etc/kubernetes/pki/sa.pub $host:/etc/kubernetes/pki/sa.pub
  scp /etc/kubernetes/pki/front-proxy-ca.crt $host:/etc/kubernetes/pki/front-proxy-ca.crt
  scp /etc/kubernetes/pki/front-proxy-ca.key $host:/etc/kubernetes/pki/front-proxy-ca.key
  scp /etc/kubernetes/pki/etcd/ca.crt $host:/etc/kubernetes/pki/etcd/ca.crt
  scp /etc/kubernetes/pki/etcd/ca.key $host:/etc/kubernetes/pki/etcd/ca.key
  scp /etc/kubernetes/admin.conf $host:/etc/kubernetes/admin.conf
done
```

- åœ¨k8s-master02ä¸ŠæŠŠèŠ‚ç‚¹åŠ å…¥é›†ç¾¤

```sh
# åˆ›å»ºç›¸å…³çš„è¯ä¹¦ä»¥åŠkubeleté…ç½®æ–‡ä»¶
$ kubeadm alpha phase certs all --config /root/kubeadm-config.yaml
$ kubeadm alpha phase kubeconfig controller-manager --config /root/kubeadm-config.yaml
$ kubeadm alpha phase kubeconfig scheduler --config /root/kubeadm-config.yaml
$ kubeadm alpha phase kubelet config write-to-disk --config /root/kubeadm-config.yaml
$ kubeadm alpha phase kubelet write-env-file --config /root/kubeadm-config.yaml
$ kubeadm alpha phase kubeconfig kubelet --config /root/kubeadm-config.yaml
$ systemctl restart kubelet

# è®¾ç½®k8s-master01ä»¥åŠk8s-master02çš„HOSTNAMEä»¥åŠåœ°å€
$ export CP0_IP=192.168.20.20
$ export CP0_HOSTNAME=k8s-master01
$ export CP1_IP=192.168.20.21
$ export CP1_HOSTNAME=k8s-master02

# etcdé›†ç¾¤æ·»åŠ èŠ‚ç‚¹
$ kubectl exec -n kube-system etcd-${CP0_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://${CP0_IP}:2379 member add ${CP1_HOSTNAME} https://${CP1_IP}:2380
$ kubeadm alpha phase etcd local --config /root/kubeadm-config.yaml

# å¯åŠ¨masterèŠ‚ç‚¹
$ kubeadm alpha phase kubeconfig all --config /root/kubeadm-config.yaml
$ kubeadm alpha phase controlplane all --config /root/kubeadm-config.yaml
$ kubeadm alpha phase mark-master --config /root/kubeadm-config.yaml

# ä¿®æ”¹/etc/kubernetes/admin.confçš„æœåŠ¡åœ°å€æŒ‡å‘æœ¬æœº
$ sed -i "s/192.168.20.20:6443/192.168.20.21:6443/g" /etc/kubernetes/admin.conf
```

- åœ¨k8s-master03ä¸ŠæŠŠèŠ‚ç‚¹åŠ å…¥é›†ç¾¤

```sh
# åˆ›å»ºç›¸å…³çš„è¯ä¹¦ä»¥åŠkubeleté…ç½®æ–‡ä»¶
$ kubeadm alpha phase certs all --config /root/kubeadm-config.yaml
$ kubeadm alpha phase kubeconfig controller-manager --config /root/kubeadm-config.yaml
$ kubeadm alpha phase kubeconfig scheduler --config /root/kubeadm-config.yaml
$ kubeadm alpha phase kubelet config write-to-disk --config /root/kubeadm-config.yaml
$ kubeadm alpha phase kubelet write-env-file --config /root/kubeadm-config.yaml
$ kubeadm alpha phase kubeconfig kubelet --config /root/kubeadm-config.yaml
$ systemctl restart kubelet

# è®¾ç½®k8s-master01ä»¥åŠk8s-master03çš„HOSTNAMEä»¥åŠåœ°å€
$ export CP0_IP=192.168.20.20
$ export CP0_HOSTNAME=k8s-master01
$ export CP2_IP=192.168.20.22
$ export CP2_HOSTNAME=k8s-master03

# etcdé›†ç¾¤æ·»åŠ èŠ‚ç‚¹
$ kubectl exec -n kube-system etcd-${CP0_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://${CP0_IP}:2379 member add ${CP2_HOSTNAME} https://${CP2_IP}:2380
$ kubeadm alpha phase etcd local --config /root/kubeadm-config.yaml

# å¯åŠ¨masterèŠ‚ç‚¹
$ kubeadm alpha phase kubeconfig all --config /root/kubeadm-config.yaml
$ kubeadm alpha phase controlplane all --config /root/kubeadm-config.yaml
$ kubeadm alpha phase mark-master --config /root/kubeadm-config.yaml

# ä¿®æ”¹/etc/kubernetes/admin.confçš„æœåŠ¡åœ°å€æŒ‡å‘æœ¬æœº
$ sed -i "s/192.168.20.20:6443/192.168.20.22:6443/g" /etc/kubernetes/admin.conf
```

- åœ¨æ‰€æœ‰masterèŠ‚ç‚¹ä¸Šå…è®¸hpaé€šè¿‡æ¥å£é‡‡é›†æ•°æ®ï¼Œä¿®æ”¹`/etc/kubernetes/manifests/kube-controller-manager.yaml`

```sh
$ vi /etc/kubernetes/manifests/kube-controller-manager.yaml
    - --horizontal-pod-autoscaler-use-rest-clients=false
```

- åœ¨æ‰€æœ‰masterä¸Šå…è®¸istioçš„è‡ªåŠ¨æ³¨å…¥ï¼Œä¿®æ”¹`/etc/kubernetes/manifests/kube-apiserver.yaml`

```sh
$ vi /etc/kubernetes/manifests/kube-apiserver.yaml
    - --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota

# é‡å¯æœåŠ¡
systemctl restart kubelet
```

- åœ¨ä»»æ„masterèŠ‚ç‚¹ä¸Šå®‰è£…calicoï¼Œå®‰è£…calicoç½‘ç»œç»„ä»¶åï¼ŒnodesçŠ¶æ€æ‰ä¼šæ¢å¤æ­£å¸¸

```sh
$ kubectl apply -f calico/
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

### masterè´Ÿè½½å‡è¡¡è®¾ç½®

#### keepalivedå®‰è£…é…ç½®

- åœ¨æ‰€æœ‰masterèŠ‚ç‚¹ä¸Šé‡å¯keepalived

```sh
$ systemctl restart keepalived
$ systemctl status keepalived

# æ£€æŸ¥keepalivedçš„vipæ˜¯å¦ç”Ÿæ•ˆ
$ curl -k https://k8s-master-lb:6443
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

#### nginxè´Ÿè½½å‡è¡¡é…ç½®

- nginxè´Ÿè½½å‡è¡¡ç”±kubeletæ‰˜ç®¡ï¼Œå¯åŠ¨kubeletä¼šè‡ªåŠ¨å¯åŠ¨nginx-lb

# éªŒè¯è´Ÿè½½å‡è¡¡çš„16443ç«¯å£æ˜¯å¦ç”Ÿæ•ˆ
$ curl -k https://k8s-master-lb:16443
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

#### kube-proxyé«˜å¯ç”¨è®¾ç½®

- åœ¨ä»»æ„masterèŠ‚ç‚¹ä¸Šè®¾ç½®kube-proxyé«˜å¯ç”¨

```sh
# ä¿®æ”¹kube-proxyçš„configmapï¼ŒæŠŠserveræŒ‡å‘load-balanceåœ°å€å’Œç«¯å£
$ kubectl edit -n kube-system configmap/kube-proxy
    server: https://192.168.20.10:16443
```

- åœ¨ä»»æ„masterèŠ‚ç‚¹ä¸Šé‡å¯kube-proxy

```sh
# æŸ¥æ‰¾å¯¹åº”çš„kube-proxy pods
$ kubectl get pods --all-namespaces -o wide | grep proxy

# åˆ é™¤å¹¶é‡å¯å¯¹åº”çš„kube-proxy pods
$ kubectl delete pod -n kube-system kube-proxy-XXX
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

#### éªŒè¯é«˜å¯ç”¨çŠ¶æ€

- åœ¨ä»»æ„masterèŠ‚ç‚¹ä¸ŠéªŒè¯æœåŠ¡å¯åŠ¨æƒ…å†µ

```sh
# æ£€æŸ¥èŠ‚ç‚¹æƒ…å†µ
$ kubectl get nodes
NAME              STATUS    ROLES     AGE       VERSION
k8s-master01   Ready     master    1h        v1.11.1
k8s-master02   Ready     master    58m       v1.11.1
k8s-master03   Ready     master    55m       v1.11.1

# æ£€æŸ¥podsè¿è¡Œæƒ…å†µ
$ kubectl get pods -n kube-system -o wide
NAME                                   READY     STATUS    RESTARTS   AGE       IP              NODE
calico-node-nxskr                      2/2       Running   0          46m       192.168.20.22   k8s-master03
calico-node-xv5xt                      2/2       Running   0          46m       192.168.20.20   k8s-master01
calico-node-zsmgp                      2/2       Running   0          46m       192.168.20.21   k8s-master02
coredns-78fcdf6894-kfzc7               1/1       Running   0          1h        172.168.2.3     k8s-master03
coredns-78fcdf6894-t957l               1/1       Running   0          46m       172.168.1.2     k8s-master02
etcd-k8s-master01                      1/1       Running   0          1h        192.168.20.20   k8s-master01
etcd-k8s-master02                      1/1       Running   0          58m       192.168.20.21   k8s-master02
etcd-k8s-master03                      1/1       Running   0          54m       192.168.20.22   k8s-master03
kube-apiserver-k8s-master01            1/1       Running   0          52m       192.168.20.20   k8s-master01
kube-apiserver-k8s-master02            1/1       Running   0          52m       192.168.20.21   k8s-master02
kube-apiserver-k8s-master03            1/1       Running   0          51m       192.168.20.22   k8s-master03
kube-controller-manager-k8s-master01   1/1       Running   0          34m       192.168.20.20   k8s-master01
kube-controller-manager-k8s-master02   1/1       Running   0          33m       192.168.20.21   k8s-master02
kube-controller-manager-k8s-master03   1/1       Running   0          33m       192.168.20.22   k8s-master03
kube-proxy-g9749                       1/1       Running   0          36m       192.168.20.22   k8s-master03
kube-proxy-lhzhb                       1/1       Running   0          35m       192.168.20.20   k8s-master01
kube-proxy-x8jwt                       1/1       Running   0          36m       192.168.20.21   k8s-master02
kube-scheduler-k8s-master01            1/1       Running   1          1h        192.168.20.20   k8s-master01
kube-scheduler-k8s-master02            1/1       Running   0          57m       192.168.20.21   k8s-master02
kube-scheduler-k8s-master03            1/1       Running   1          54m       192.168.20.22   k8s-master03
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

#### åŸºç¡€ç»„ä»¶å®‰è£…

- åœ¨ä»»æ„masterèŠ‚ç‚¹ä¸Šå…è®¸masterä¸Šéƒ¨ç½²pod

```sh
$ kubectl taint nodes --all node-role.kubernetes.io/master-
```

- åœ¨ä»»æ„masterèŠ‚ç‚¹ä¸Šå®‰è£…metrics-serverï¼Œä»v1.11.0å¼€å§‹ï¼Œæ€§èƒ½é‡‡é›†ä¸å†é‡‡ç”¨heapsteré‡‡é›†podæ€§èƒ½æ•°æ®ï¼Œè€Œæ˜¯ä½¿ç”¨metrics-server

```sh
$ kubectl apply -f metrics-server/

# ç­‰å¾…5åˆ†é’Ÿï¼ŒæŸ¥çœ‹æ€§èƒ½æ•°æ®æ˜¯å¦æ­£å¸¸æ”¶é›†
$ kubectl top pods -n kube-system
NAME                                    CPU(cores)   MEMORY(bytes)
calico-node-wkstv                       47m          113Mi
calico-node-x2sn5                       36m          104Mi
calico-node-xnh6s                       32m          106Mi
coredns-78fcdf6894-2xc6s                14m          30Mi
coredns-78fcdf6894-rk6ch                10m          22Mi
kube-apiserver-k8s-master01             163m         816Mi
kube-apiserver-k8s-master02             79m          617Mi
kube-apiserver-k8s-master03             73m          614Mi
kube-controller-manager-k8s-master01    52m          141Mi
kube-controller-manager-k8s-master02    0m           14Mi
kube-controller-manager-k8s-master03    0m           13Mi
kube-proxy-269t2                        4m           21Mi
kube-proxy-6jc8n                        9m           37Mi
kube-proxy-7n8xb                        9m           39Mi
kube-scheduler-k8s-master01             20m          25Mi
kube-scheduler-k8s-master02             15m          19Mi
kube-scheduler-k8s-master03             15m          19Mi
metrics-server-77b77f5fc6-jm8t6         3m           43Mi
```

- åœ¨ä»»æ„masterèŠ‚ç‚¹ä¸Šå®‰è£…heapsterï¼Œä»v1.11.0å¼€å§‹ï¼Œæ€§èƒ½é‡‡é›†ä¸å†é‡‡ç”¨heapsteré‡‡é›†podæ€§èƒ½æ•°æ®ï¼Œè€Œæ˜¯ä½¿ç”¨metrics-serverï¼Œä½†æ˜¯dashboardä¾ç„¶ä½¿ç”¨heapsterå‘ˆç°æ€§èƒ½æ•°æ®

```sh
# å®‰è£…heapsterï¼Œéœ€è¦ç­‰å¾…5åˆ†é’Ÿï¼Œç­‰å¾…æ€§èƒ½æ•°æ®é‡‡é›†
$ kubectl apply -f heapster/
```

- åœ¨ä»»æ„masterèŠ‚ç‚¹ä¸Šå®‰è£…dashboard

```sh
# å®‰è£…dashboard
$ kubectl apply -f dashboard/
```

> æˆåŠŸå®‰è£…åè®¿é—®ä»¥ä¸‹ç½‘å€æ‰“å¼€dashboardçš„ç™»å½•ç•Œé¢ï¼Œè¯¥ç•Œé¢æç¤ºéœ€è¦ç™»å½•token: https://k8s-master-lb:30000/

![dashboard-login](images/dashboard-login.png)

- åœ¨ä»»æ„masterèŠ‚ç‚¹ä¸Šè·å–dashboardçš„ç™»å½•token

```sh
# è·å–dashboardçš„ç™»å½•token
$ kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
```

> ä½¿ç”¨tokenè¿›è¡Œç™»å½•ï¼Œè¿›å…¥åå¯ä»¥çœ‹åˆ°heapsteré‡‡é›†çš„å„ä¸ªpodä»¥åŠèŠ‚ç‚¹çš„æ€§èƒ½æ•°æ®

![dashboard](images/dashboard.png)

- åœ¨ä»»æ„masterèŠ‚ç‚¹ä¸Šå®‰è£…traefik

```sh
# åˆ›å»ºk8s-master-lbåŸŸåçš„è¯ä¹¦
$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj "/CN=k8s-master-lb"

# æŠŠè¯ä¹¦å†™å…¥åˆ°secret
kubectl -n kube-system create secret generic traefik-cert --from-file=tls.key --from-file=tls.crt

# å®‰è£…traefik
$ kubectl apply -f traefik/
```

> æˆåŠŸå®‰è£…åè®¿é—®ä»¥ä¸‹ç½‘å€æ‰“å¼€traefikç®¡ç†ç•Œé¢: http://k8s-master-lb:30011/

![traefik](images/traefik.png)

- åœ¨ä»»æ„masterèŠ‚ç‚¹ä¸Šå®‰è£…istio

```sh
# å®‰è£…istio
$ kubectl apply -f istio/

# æ£€æŸ¥istioæœåŠ¡ç›¸å…³pods
$ kubectl get pods -n istio-system
NAME                                        READY     STATUS      RESTARTS   AGE
grafana-69c856fc69-jbx49                    1/1       Running     1          21m
istio-citadel-7c4fc8957b-vdbhp              1/1       Running     1          21m
istio-cleanup-secrets-5g95n                 0/1       Completed   0          21m
istio-egressgateway-64674bd988-44fg8        1/1       Running     0          18m
istio-egressgateway-64674bd988-dgvfm        1/1       Running     1          16m
istio-egressgateway-64674bd988-fprtc        1/1       Running     0          18m
istio-egressgateway-64674bd988-kl6pw        1/1       Running     3          16m
istio-egressgateway-64674bd988-nphpk        1/1       Running     3          16m
istio-galley-595b94cddf-c5ctw               1/1       Running     70         21m
istio-grafana-post-install-nhs47            0/1       Completed   0          21m
istio-ingressgateway-4vtk5                  1/1       Running     2          21m
istio-ingressgateway-5rscp                  1/1       Running     3          21m
istio-ingressgateway-6z95f                  1/1       Running     3          21m
istio-policy-589977bff5-jx5fd               2/2       Running     3          21m
istio-policy-589977bff5-n74q8               2/2       Running     3          21m
istio-sidecar-injector-86c4d57d56-mfnbp     1/1       Running     39         21m
istio-statsd-prom-bridge-5698d5798c-xdpp6   1/1       Running     1          21m
istio-telemetry-85d6475bfd-8lvsm            2/2       Running     2          21m
istio-telemetry-85d6475bfd-bfjsn            2/2       Running     2          21m
istio-telemetry-85d6475bfd-d9ld9            2/2       Running     2          21m
istio-tracing-bd5765b5b-cmszp               1/1       Running     1          21m
prometheus-77c5fc7cd-zf7zr                  1/1       Running     1          21m
servicegraph-6b99c87849-l6zm6               1/1       Running     1          21m
```

- åœ¨ä»»æ„masterèŠ‚ç‚¹ä¸Šå®‰è£…prometheus

```sh
# å®‰è£…prometheus
$ kubectl apply -f prometheus/
```

> æˆåŠŸå®‰è£…åè®¿é—®ä»¥ä¸‹ç½‘å€æ‰“å¼€prometheusç®¡ç†ç•Œé¢ï¼ŒæŸ¥çœ‹ç›¸å…³æ€§èƒ½é‡‡é›†æ•°æ®: http://k8s-master-lb:30013/

![prometheus](images/prometheus.png)

> æˆåŠŸå®‰è£…åè®¿é—®ä»¥ä¸‹ç½‘å€æ‰“å¼€grafanaç®¡ç†ç•Œé¢(è´¦å·å¯†ç éƒ½æ˜¯`admin`)ï¼ŒæŸ¥çœ‹ç›¸å…³æ€§èƒ½é‡‡é›†æ•°æ®: http://k8s-master-lb:30006/
> ç™»å½•åï¼Œè¿›å…¥datasourceè®¾ç½®ç•Œé¢ï¼Œå¢åŠ prometheusæ•°æ®æºï¼Œhttp://k8s-master-lb:30006/datasources

![grafana-datasource](images/grafana-datasource.png)

> è¿›å…¥å¯¼å…¥dashboardç•Œé¢: http://k8s-master-lb:30006/dashboard/import å¯¼å…¥`heapster/grafana-dashboard`ç›®å½•ä¸‹çš„dashboard `Kubernetes App Metrics`å’Œ`Kubernetes cluster monitoring (via Prometheus)`

![grafana-import](images/grafana-import.png)

> å¯¼å…¥çš„dashboardæ€§èƒ½å‘ˆç°å¦‚ä¸‹å›¾:

![grafana-cluster](images/grafana-cluster.png)

![grafana-app](images/grafana-app.png)

---

[è¿”å›ç›®å½•](#ç›®å½•)

### workerèŠ‚ç‚¹è®¾ç½®

#### workeråŠ å…¥é«˜å¯ç”¨é›†ç¾¤

- åœ¨æ‰€æœ‰workersèŠ‚ç‚¹ä¸Šï¼Œä½¿ç”¨kubeadm joinåŠ å…¥kubernetesé›†ç¾¤

```sh
# æ¸…ç†èŠ‚ç‚¹ä¸Šçš„kubernetesé…ç½®ä¿¡æ¯
$ kubeadm reset

# ä½¿ç”¨ä¹‹å‰kubeadm initæ‰§è¡Œç»“æœè®°å½•çš„${YOUR_TOKEN}ä»¥åŠ${YOUR_DISCOVERY_TOKEN_CA_CERT_HASH}ï¼ŒæŠŠworkerèŠ‚ç‚¹åŠ å…¥åˆ°é›†ç¾¤
$ kubeadm join 192.168.20.20:6443 --token ${YOUR_TOKEN} --discovery-token-ca-cert-hash sha256:${YOUR_DISCOVERY_TOKEN_CA_CERT_HASH}


# åœ¨workersä¸Šä¿®æ”¹kubernetesé›†ç¾¤è®¾ç½®ï¼Œè®©serveræŒ‡å‘nginxè´Ÿè½½å‡è¡¡çš„ipå’Œç«¯å£
$ sed -i "s/192.168.20.20:6443/192.168.20.10:16443/g" /etc/kubernetes/bootstrap-kubelet.conf
$ sed -i "s/192.168.20.20:6443/192.168.20.10:16443/g" /etc/kubernetes/kubelet.conf

# é‡å¯æœ¬èŠ‚ç‚¹
$ systemctl restart docker kubelet
```

- åœ¨ä»»æ„masterèŠ‚ç‚¹ä¸ŠéªŒè¯èŠ‚ç‚¹çŠ¶æ€

```sh
$ kubectl get nodes
NAME           STATUS    ROLES     AGE       VERSION
k8s-master01   Ready     master    1h        v1.11.1
k8s-master02   Ready     master    58m       v1.11.1
k8s-master03   Ready     master    55m       v1.11.1
k8s-node01     Ready     <none>    30m       v1.11.1
k8s-node02     Ready     <none>    24m       v1.11.1
k8s-node03     Ready     <none>    22m       v1.11.1
k8s-node04     Ready     <none>    22m       v1.11.1
k8s-node05     Ready     <none>    16m       v1.11.1
k8s-node06     Ready     <none>    13m       v1.11.1
k8s-node07     Ready     <none>    11m       v1.11.1
k8s-node08     Ready     <none>    10m       v1.11.1
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

### é›†ç¾¤éªŒè¯

#### éªŒè¯é›†ç¾¤é«˜å¯ç”¨è®¾ç½®

- éªŒè¯é›†ç¾¤é«˜å¯ç”¨

```sh
# åˆ›å»ºä¸€ä¸ªreplicas=3çš„nginx deployment
$ kubectl run nginx --image=nginx --replicas=3 --port=80
deployment "nginx" created

# æ£€æŸ¥nginx podçš„åˆ›å»ºæƒ…å†µ
$ kubectl get pods -l=run=nginx -o wide
NAME                     READY     STATUS    RESTARTS   AGE       IP             NODE
nginx-58b94844fd-jvlqh   1/1       Running   0          9s        172.168.7.2    k8s-node05
nginx-58b94844fd-mkt72   1/1       Running   0          9s        172.168.9.2    k8s-node07
nginx-58b94844fd-xhb8x   1/1       Running   0          9s        172.168.11.2   k8s-node09

# åˆ›å»ºnginxçš„NodePort service
$ kubectl expose deployment nginx --type=NodePort --port=80
service "nginx" exposed

# æ£€æŸ¥nginx serviceçš„åˆ›å»ºæƒ…å†µ
$ kubectl get svc -l=run=nginx -o wide
NAME      TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE       SELECTOR
nginx     NodePort   10.106.129.121   <none>        80:31443/TCP   7s        run=nginx

# æ£€æŸ¥nginx NodePort serviceæ˜¯å¦æ­£å¸¸æä¾›æœåŠ¡
$ curl k8s-master-lb:31443
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

- podä¹‹é—´äº’è®¿æµ‹è¯•

```sh
# å¯åŠ¨ä¸€ä¸ªclientæµ‹è¯•nginxæ˜¯å¦å¯ä»¥è®¿é—®
kubectl run nginx-client -ti --rm --image=alpine -- ash
/ # wget -O - nginx
Connecting to nginx (10.102.101.78:80)
index.html           100% |*****************************************|   612   0:00:00 ETA

<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

# æ¸…é™¤nginxçš„deploymentä»¥åŠservice
kubectl delete deploy,svc nginx
```

- æµ‹è¯•HPAè‡ªåŠ¨æ‰©å±•

```sh
# åˆ›å»ºæµ‹è¯•æœåŠ¡
kubectl run nginx-server --requests=cpu=10m --image=nginx --port=80
kubectl expose deployment nginx-server --port=80

# åˆ›å»ºhpa
kubectl autoscale deployment nginx-server --cpu-percent=10 --min=1 --max=10
kubectl get hpa
kubectl describe hpa nginx-server

# ç»™æµ‹è¯•æœåŠ¡å¢åŠ è´Ÿè½½
kubectl run -ti --rm load-generator --image=busybox -- ash
wget -q -O- http://nginx-server.default.svc.cluster.local > /dev/null
while true; do wget -q -O- http://nginx-server.default.svc.cluster.local > /dev/null; done

# æ£€æŸ¥hpaè‡ªåŠ¨æ‰©å±•æƒ…å†µï¼Œä¸€èˆ¬éœ€è¦ç­‰å¾…å‡ åˆ†é’Ÿã€‚ç»“æŸå¢åŠ è´Ÿè½½åï¼Œpodè‡ªåŠ¨ç¼©å®¹ï¼ˆè‡ªåŠ¨ç¼©å®¹éœ€è¦å¤§æ¦‚10-15åˆ†é’Ÿï¼‰
kubectl get hpa -w

# åˆ é™¤æµ‹è¯•æ•°æ®
kubectl delete deploy,svc,hpa nginx-server
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

- è‡³æ­¤kubernetesé«˜å¯ç”¨é›†ç¾¤å®Œæˆéƒ¨ç½²ï¼Œå¹¶æµ‹è¯•é€šè¿‡ ğŸ˜ƒ

[è¿”å›ç›®å½•](#ç›®å½•)

### é›†ç¾¤å‡çº§

#### é›†ç¾¤å‡çº§v1.11.1å‡çº§v1.11.5

- Kubernetesæœ€è¿‘çˆ†å‡ºé«˜å±å®‰å…¨æ¼æ´ï¼ˆCVE-2018-1002105ï¼‰ï¼Œv1.11.xå»ºè®®å‡çº§åˆ°v1.11.5æˆ–æ›´æ–°ç‰ˆæœ¬ä»¥ä¿®å¤æ¼æ´ï¼Œè¯¦ç»†å‚è§: https://thenewstack.io/critical-vulnerability-allows-kubernetes-node-hacking/

- åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šæ›´æ–°kubeletå’Œkubeadmåˆ°v1.11.5

```
# æ›´æ–°kubeletå’Œkubeadmåˆ°v1.11.5
$ yum -y update kubeadm-1.11.5-0.x86_64 kubelet-1.11.5-0.x86_64

# é‡å¯æœåŠ¡
$ systemctl daemon-reload
$ systemctl restart kubelet
```

- åœ¨æ‰€æœ‰èŠ‚ç‚¹ä¸Šæ‹‰å–v1.11.5çš„kubernetesé•œåƒ

```
$ docker pull k8s.gcr.io/kube-controller-manager-amd64:v1.11.5
$ docker pull k8s.gcr.io/kube-scheduler-amd64:v1.11.5
$ docker pull k8s.gcr.io/kube-apiserver-amd64:v1.11.5
$ docker pull k8s.gcr.io/kube-proxy-amd64:v1.11.5
```

- åœ¨æ‰€æœ‰masterèŠ‚ç‚¹ä¸Šè¿›è¡Œç‰ˆæœ¬æ›´æ–°

```
# æŸ¥çœ‹ç‰ˆæœ¬æ›´æ–°çš„å„ä¸ªæ¨¡å—çš„æ”¯æŒæƒ…å†µ
$ kubeadm upgrade plan
Upgrade to the latest stable version:

COMPONENT            CURRENT   AVAILABLE
API Server           v1.11.1   v1.11.5
Controller Manager   v1.11.1   v1.11.5
Scheduler            v1.11.1   v1.11.5
Kube Proxy           v1.11.1   v1.11.5
CoreDNS              1.1.3     1.1.3
Etcd                 3.2.18    3.2.18

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.11.5

# æ­£å¼æ‰§è¡Œæ›´æ–°ï¼Œè¾“å‡ºå¦‚ä¸‹ï¼š
$ kubeadm upgrade apply v1.11.5
[preflight] Running pre-flight checks.
[upgrade] Making sure the cluster is healthy:
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
[upgrade/apply] Respecting the --cri-socket flag that is set with higher priority than the config file.
[upgrade/version] You have chosen to change the cluster version to "v1.11.5"
[upgrade/versions] Cluster version: v1.11.1
[upgrade/versions] kubeadm version: v1.11.5
[upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y
[upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd]
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.11.5"...
Static pod: kube-apiserver-pro-master01 hash: f8a81b3b047edadfaea2759697caf09e
Static pod: kube-controller-manager-pro-master01 hash: 94369a77f84beef59df8e6c0c075d6eb
Static pod: kube-scheduler-pro-master01 hash: 537879acc30dd5eff5497cb2720a6d64
[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests249561254"
[controlplane] wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests249561254/kube-apiserver.yaml"
[controlplane] wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests249561254/kube-controller-manager.yaml"
[controlplane] wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests249561254/kube-scheduler.yaml"
[certificates] Using the existing etcd/ca certificate and key.
[certificates] Using the existing apiserver-etcd-client certificate and key.
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-12-05-18-26-14/kube-apiserver.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
Static pod: kube-apiserver-pro-master01 hash: f8a81b3b047edadfaea2759697caf09e
Static pod: kube-apiserver-pro-master01 hash: 145a58c8db4210f1eef7891f55dc6db6
[apiclient] Found 3 Pods for label selector component=kube-apiserver
[upgrade/staticpods] Component "kube-apiserver" upgraded successfully!
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-12-05-18-26-14/kube-controller-manager.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
Static pod: kube-controller-manager-pro-master01 hash: 94369a77f84beef59df8e6c0c075d6eb
Static pod: kube-controller-manager-pro-master01 hash: c0de2763a74e6511dd773bffaec3a971
[apiclient] Found 3 Pods for label selector component=kube-controller-manager
[upgrade/staticpods] Component "kube-controller-manager" upgraded successfully!
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-12-05-18-26-14/kube-scheduler.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
Static pod: kube-scheduler-pro-master01 hash: 537879acc30dd5eff5497cb2720a6d64
Static pod: kube-scheduler-pro-master01 hash: 03ccb6e070f017ec5bf3aea2233e9c9e
[apiclient] Found 3 Pods for label selector component=kube-scheduler
[upgrade/staticpods] Component "kube-scheduler" upgraded successfully!
[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.11" in namespace kube-system with the configuration for the kubelets in the cluster
[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.11" ConfigMap in the kube-system namespace
[kubelet] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "pro-master01" as an annotation
[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.11.5". Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
```

- æ£€æŸ¥èŠ‚ç‚¹æ›´æ–°æƒ…å†µ

```
# æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹çš„VERSION
$ kubectl get no
NAME           STATUS    ROLES     AGE       VERSION
k8s-master01   Ready     master    43d       v1.11.5
k8s-master02   Ready     master    43d       v1.11.5
k8s-master03   Ready     master    43d       v1.11.5
k8s-node01     Ready     <none>    42d       v1.11.5
k8s-node02     Ready     <none>    43d       v1.11.5
k8s-node03     Ready     <none>    43d       v1.11.5
k8s-node04     Ready     <none>    43d       v1.11.5
k8s-node05     Ready     <none>    43d       v1.11.5
k8s-node06     Ready     <none>    43d       v1.11.5
k8s-node07     Ready     <none>    43d       v1.11.5
k8s-node08     Ready     <none>    43d       v1.11.5

# æ£€æŸ¥ç›¸å…³podé•œåƒæ˜¯å¦å·²ç»æ›´æ–°
$ kubectl get po -n kube-system -o yaml | grep "image:" | grep "kube-"
      image: k8s.gcr.io/kube-apiserver-amd64:v1.11.5
      image: k8s.gcr.io/kube-apiserver-amd64:v1.11.5
      image: k8s.gcr.io/kube-apiserver-amd64:v1.11.5
      image: k8s.gcr.io/kube-controller-manager-amd64:v1.11.5
      image: k8s.gcr.io/kube-controller-manager-amd64:v1.11.5
      image: k8s.gcr.io/kube-controller-manager-amd64:v1.11.5
      image: k8s.gcr.io/kube-proxy-amd64:v1.11.5
      image: k8s.gcr.io/kube-proxy-amd64:v1.11.5
      image: k8s.gcr.io/kube-proxy-amd64:v1.11.5
      image: k8s.gcr.io/kube-proxy-amd64:v1.11.5
      image: k8s.gcr.io/kube-proxy-amd64:v1.11.5
      image: k8s.gcr.io/kube-proxy-amd64:v1.11.5
      image: k8s.gcr.io/kube-proxy-amd64:v1.11.5
      image: k8s.gcr.io/kube-proxy-amd64:v1.11.5
      image: k8s.gcr.io/kube-proxy-amd64:v1.11.5
      image: k8s.gcr.io/kube-proxy-amd64:v1.11.5
      image: k8s.gcr.io/kube-proxy-amd64:v1.11.5
      image: k8s.gcr.io/kube-scheduler-amd64:v1.11.5
      image: k8s.gcr.io/kube-scheduler-amd64:v1.11.5
      image: k8s.gcr.io/kube-scheduler-amd64:v1.11.5
```

- è‡³æ­¤kubernetesé«˜å¯ç”¨é›†ç¾¤å®Œæˆåˆ°v1.11.5çš„å‡çº§ ğŸ˜ƒ

[è¿”å›ç›®å½•](#ç›®å½•)
