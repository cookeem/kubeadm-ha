# é€šè¿‡kubeadmå®‰è£…kubernetesé«˜å¯ç”¨é›†ç¾¤

- å®¹å™¨è¿è¡Œæ—¶ä½¿ç”¨docker
- é€‚ç”¨kubernetesç‰ˆæœ¬: v1.24.xä»¥ä¸Šç‰ˆæœ¬

- [ä¸­æ–‡](README.md)
- [English](README-EN.md)

## éƒ¨ç½²èŠ‚ç‚¹ä¿¡æ¯

hostname     | ip address      | comment   
:---         | :---            | :---      
k8s-master01 | 192.168.0.101   | kubernetes æ§åˆ¶å¹³é¢ä¸»æœº master01
k8s-master02 | 192.168.0.102   | kubernetes æ§åˆ¶å¹³é¢ä¸»æœº master02
k8s-master03 | 192.168.0.103   | kubernetes æ§åˆ¶å¹³é¢ä¸»æœº master03
k8s-vip      | 192.168.0.100   | kubernetes æµ®åŠ¨IPï¼Œé€šè¿‡keepalivedåˆ›å»ºï¼Œå¦‚æœä½¿ç”¨å…¬æœ‰äº‘è¯·é¢„å…ˆç”³è¯·è¯¥æµ®åŠ¨IP

```bash
# å„èŠ‚ç‚¹è¯·æ·»åŠ ä¸»æœºåè§£é‡Š
cat << EOF >> /etc/hosts
192.168.0.100    k8s-vip
192.168.0.101    k8s-master01
192.168.0.102    k8s-master02
192.168.0.103    k8s-master03
EOF
```

## æ¶æ„è¯´æ˜

![](images/kubeadm-ha.png)

- æ¼”ç¤ºéœ€è¦ï¼Œåªéƒ¨ç½²3ä¸ªé«˜å¯ç”¨çš„masterèŠ‚ç‚¹
- ä½¿ç”¨keepalivedå’Œnginxä½œä¸ºé«˜å¯ç”¨çš„è´Ÿè½½å‡è¡¡å™¨ï¼Œé€šè¿‡dorycliå‘½ä»¤è¡Œå·¥å…·ç”Ÿæˆè´Ÿè½½å‡è¡¡å™¨çš„é…ç½®ï¼Œå¹¶é€šè¿‡docker-composeéƒ¨ç½²è´Ÿè½½å‡è¡¡å™¨
- å®¹å™¨è¿è¡Œæ—¶ä½¿ç”¨dockerï¼Œcri-socketä½¿ç”¨cri-dockerdè¿æ¥dockerå’Œkubernetes

## ç‰ˆæœ¬ä¿¡æ¯

```bash
# æ“ä½œç³»ç»Ÿç‰ˆæœ¬: Debian 11
$ lsb_release -a
No LSB modules are available.
Distributor ID:     Debian
Description:        Debian GNU/Linux 11 (bullseye)
Release:            11
Codename:           bullseye

# dockerç‰ˆæœ¬: 24.0.5
$ docker version
Client: Docker Engine - Community
 Version:           24.0.5
 API version:       1.43
 Go version:        go1.20.6
 Git commit:        ced0996
 Built:             Fri Jul 21 20:35:45 2023
 OS/Arch:           linux/amd64
 Context:           default

Server: Docker Engine - Community
 Engine:
  Version:          24.0.5
  API version:      1.43 (minimum version 1.12)
  Go version:       go1.20.6
  Git commit:       a61e2b4
  Built:            Fri Jul 21 20:35:45 2023
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.6.22
  GitCommit:        8165feabfdfe38c65b599c4993d227328c231fca
 runc:
  Version:          1.1.8
  GitCommit:        v1.1.8-0-g82f18fe
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0

# cri-dockerdç‰ˆæœ¬: 0.3.4
$ cri-dockerd --version
cri-dockerd 0.3.4 (e88b1605)

# dorycliç‰ˆæœ¬: v1.5.0
$ dorycli version
dorycli version: v1.5.0
install dory-engine version: v2.5.0
install dory-console version: v2.5.0


# kubeadmç‰ˆæœ¬: v1.28.0
$ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.0", GitCommit:"855e7c48de7388eb330da0f8d9d2394ee818fb8d", GitTreeState:"clean", BuildDate:"2023-08-15T10:20:15Z", GoVersion:"go1.20.7", Compiler:"gc", Platform:"linux/amd64"}

# kubernetesç‰ˆæœ¬: v1.28.0
$ kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
k8s-master01   Ready    control-plane   35m   v1.28.0
k8s-master02   Ready    control-plane   31m   v1.28.0
k8s-master03   Ready    control-plane   30m   v1.28.0
```

## å®‰è£…docker

- åœ¨æ‰€æœ‰èŠ‚ç‚¹å®‰è£…dockeræœåŠ¡

```bash
# å®‰è£…åŸºç¡€è½¯ä»¶
apt-get update
apt-get install -y sudo wget ca-certificates curl gnupg htop git jq tree

# å®‰è£…docker
install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
chmod a+r /etc/apt/keyrings/docker.gpg
echo   "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/debian \
  "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" |   tee /etc/apt/sources.list.d/docker.list > /dev/null
apt-get update
apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-compose

# æ£€æŸ¥dockerç‰ˆæœ¬
docker version

# è®¾ç½®dockerå‚æ•°
cat << EOF > /etc/docker/daemon.json
{
    "exec-opts": ["native.cgroupdriver=systemd"],
    "log-driver": "json-file",
    "log-opts": {
        "max-size": "100m"
    },
    "storage-driver": "overlay2"
}
EOF

# é‡å¯dockeræœåŠ¡
systemctl restart docker
systemctl status docker

# éªŒè¯dockeræœåŠ¡æ˜¯å¦æ­£å¸¸
docker images
docker pull busybox
docker run --rm busybox uname -m
```

## å®‰è£…kubernetes

- åœ¨æ‰€æœ‰èŠ‚ç‚¹å®‰è£…kubernetesç›¸å…³è½¯ä»¶

```bash
# å®‰è£…kubernetesç›¸å…³ç»„ä»¶
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
cat <<EOF >/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
apt-get update
apt-get install -y kubelet kubeadm kubectl
kubeadm version

# è·å–kubernetesæ‰€éœ€è¦çš„é•œåƒ
kubeadm config images list --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers
export PAUSE_IMAGE=$(kubeadm config images list --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers | grep pause)

# æ³¨æ„pauseé•œåƒç”¨äºé…ç½®cri-dockerdçš„å¯åŠ¨å‚æ•°
# åº”è¯¥æ˜¯è¾“å‡º registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9
echo $PAUSE_IMAGE

# å®‰è£…cri-dockerdï¼Œç”¨äºè¿æ¥kuberneteså’Œdocker
wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.4/cri-dockerd-0.3.4.amd64.tgz
tar zxvf cri-dockerd-0.3.4.amd64.tgz 
cd cri-dockerd/
mkdir -p /usr/local/bin
install -o root -g root -m 0755 cri-dockerd /usr/local/bin/cri-dockerd

# åˆ›å»ºcri-docker.socketå¯åŠ¨æ–‡ä»¶
cat << EOF > /etc/systemd/system/cri-docker.socket
[Unit]
Description=CRI Docker Socket for the API
PartOf=cri-docker.service

[Socket]
ListenStream=%t/cri-dockerd.sock
SocketMode=0660
SocketUser=root
SocketGroup=docker

[Install]
WantedBy=sockets.target
EOF

# åˆ›å»ºcri-docker.serviceå¯åŠ¨æ–‡ä»¶
# æ³¨æ„è®¾ç½®pauseå®¹å™¨é•œåƒä¿¡æ¯ --pod-infra-container-image=$PAUSE_IMAGE
cat << EOF > /etc/systemd/system/cri-docker.service
[Unit]
Description=CRI Interface for Docker Application Container Engine
Documentation=https://docs.mirantis.com
After=network-online.target firewalld.service docker.service
Wants=network-online.target
Requires=cri-docker.socket

[Service]
Type=notify
ExecStart=/usr/local/bin/cri-dockerd --container-runtime-endpoint fd:// --pod-infra-container-image=$PAUSE_IMAGE
ExecReload=/bin/kill -s HUP \$MAINPID
TimeoutSec=0
RestartSec=2
Restart=always

# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
# Both the old, and new location are accepted by systemd 229 and up, so using the old location
# to make them work for either version of systemd.
StartLimitBurst=3

# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
# this option work for either version of systemd.
StartLimitInterval=60s

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Comment TasksMax if your systemd version does not support it.
# Only systemd 226 and above support this option.
TasksMax=infinity
Delegate=yes
KillMode=process

[Install]
WantedBy=multi-user.target
EOF

# å¯åŠ¨cri-dockerd
systemctl daemon-reload
systemctl enable --now cri-docker.socket
systemctl restart cri-docker
systemctl status cri-docker

# é€šè¿‡kubeadmé¢„å…ˆæ‹‰å–æ‰€éœ€çš„å®¹å™¨é•œåƒ
kubeadm config images pull --image-repository registry.cn-hangzhou.aliyuncs.com/google_containers --cri-socket unix:///var/run/cri-dockerd.sock
docker images
```

- åœ¨k8s-master01èŠ‚ç‚¹é€šè¿‡dorycliåˆ›å»ºå¹¶å¯åŠ¨é«˜å¯ç”¨è´Ÿè½½å‡è¡¡å™¨: keepalived, nginx-lb
- doryclié¡¹ç›®åœ°å€: [https://github.com/dory-engine/dorycli](https://github.com/dory-engine/dorycli)

```bash
# å®‰è£…dorycli
cd /root
wget https://github.com/dory-engine/dorycli/releases/download/v1.5.0/dorycli-v1.5.0-linux-amd64.tgz
tar zxvf dorycli-v1.5.0-linux-amd64.tgz
chmod a+x dorycli
mv dorycli /usr/bin/

# è®¾ç½®dorycliçš„è‡ªåŠ¨å®Œæˆï¼Œå¯ä»¥é€šè¿‡é”®ç›˜TABé”®è‡ªåŠ¨è¡¥å…¨å­å‘½ä»¤å’Œå‚æ•°
dorycli completion bash -h
source <(dorycli completion bash)
dorycli completion bash > /etc/bash_completion.d/dorycli

# ä½¿ç”¨dorycliæ‰“å°é«˜å¯ç”¨è´Ÿè½½å‡è¡¡å™¨é…ç½®ä¿¡æ¯ï¼Œå¹¶ä¿å­˜åˆ°kubeadm-ha.yaml
dorycli install ha print --language zh > kubeadm-ha.yaml

# æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹kubeadm-ha.yamlçš„é…ç½®ä¿¡æ¯
# å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤è·å–å„ä¸ªä¸»æœºçš„ç½‘å¡åå­—
ip address

# æœ¬ä¾‹å­çš„é…ç½®å¦‚ä¸‹ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹é…ç½®
cat kubeadm-ha.yaml
# éœ€è¦å®‰è£…çš„kubernetesçš„ç‰ˆæœ¬
version: "v1.28.0"
# kubernetesçš„é•œåƒä»“åº“è®¾ç½®ï¼Œå¦‚æœä¸è®¾ç½®ï¼Œé‚£ä¹ˆä½¿ç”¨å®˜æ–¹çš„é»˜è®¤é•œåƒä»“åº“
imageRepository: "registry.cn-hangzhou.aliyuncs.com/google_containers"
# ä½¿ç”¨keepalivedåˆ›å»ºçš„é«˜å¯ç”¨kubernetesé›†ç¾¤çš„æµ®åŠ¨ipåœ°å€
virtualIp: 192.168.0.100
# ä½¿ç”¨nginxæ˜ å°„çš„é«˜å¯ç”¨kubernetesé›†ç¾¤çš„apiserveræ˜ å°„ç«¯å£
virtualPort: 16443
# æµ®åŠ¨ipåœ°å€æ˜ å°„çš„ä¸»æœºåï¼Œè¯·åœ¨/etc/hostsé…ç½®æ–‡ä»¶ä¸­è¿›è¡Œä¸»æœºåæ˜ å°„è®¾ç½®
virtualHostname: k8s-vip
# kubernetesçš„å®¹å™¨è¿è¡Œæ—¶socket
# dockeræƒ…å†µä¸‹: unix:///var/run/cri-dockerd.sock
# containerdæƒ…å†µä¸‹: unix:///var/run/containerd/containerd.sock
# cri-oæƒ…å†µä¸‹: unix:///var/run/crio/crio.sock
criSocket: unix:///var/run/cri-dockerd.sock
# kubernetesé›†ç¾¤çš„podå­ç½‘åœ°å€ï¼Œå¦‚æœä¸è®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤çš„podå­ç½‘åœ°å€
podSubnet: "10.244.0.0/24"
# kubernetesé›†ç¾¤çš„serviceå­ç½‘åœ°å€ï¼Œå¦‚æœä¸è®¾ç½®ï¼Œä½¿ç”¨é»˜è®¤çš„serviceå­ç½‘åœ°å€
serviceSubnet: "10.96.0.0/16"
# keepalivedçš„é‰´æƒå¯†ç ï¼Œå¦‚æœä¸è®¾ç½®é‚£ä¹ˆä½¿ç”¨éšæœºç”Ÿæˆçš„å¯†ç 
keepAlivedAuthPass: ""
# kubernetesçš„controlplaneæ§åˆ¶å¹³é¢çš„ä¸»æœºé…ç½®ï¼Œé«˜å¯ç”¨masterèŠ‚ç‚¹æ•°é‡å¿…é¡»ä¸ºå•æ•°å¹¶ä¸”è‡³å°‘3å°
masterHosts:
    # masterèŠ‚ç‚¹çš„ä¸»æœºåï¼Œè¯·åœ¨/etc/hostsé…ç½®æ–‡ä»¶ä¸­è¿›è¡Œä¸»æœºåæ˜ å°„è®¾ç½®
  - hostname: k8s-master01
    # masterèŠ‚ç‚¹çš„IPåœ°å€
    ipAddress: 192.168.0.101
    # masterèŠ‚ç‚¹äº’è®¿ä½¿ç”¨çš„ç½‘å¡åå­—ï¼Œç”¨äºkeepalivedç½‘å¡ç»‘å®š
    networkInterface: eth0
    # keepalivedé€‰ä¸¾ä¼˜å…ˆçº§ï¼Œæ•°å€¼è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜ï¼Œå„ä¸ªmasterèŠ‚ç‚¹çš„ä¼˜å…ˆçº§ä¸èƒ½ä¸€æ ·
    keepalivedPriority: 120
    # masterèŠ‚ç‚¹çš„ä¸»æœºåï¼Œè¯·åœ¨/etc/hostsé…ç½®æ–‡ä»¶ä¸­è¿›è¡Œä¸»æœºåæ˜ å°„è®¾ç½®
  - hostname: k8s-master02
    # masterèŠ‚ç‚¹çš„IPåœ°å€
    ipAddress: 192.168.0.102
    # masterèŠ‚ç‚¹äº’è®¿ä½¿ç”¨çš„ç½‘å¡åå­—ï¼Œç”¨äºkeepalivedç½‘å¡ç»‘å®š
    networkInterface: eth0
    # keepalivedé€‰ä¸¾ä¼˜å…ˆçº§ï¼Œæ•°å€¼è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜ï¼Œå„ä¸ªmasterèŠ‚ç‚¹çš„ä¼˜å…ˆçº§ä¸èƒ½ä¸€æ ·
    keepalivedPriority: 110
    # masterèŠ‚ç‚¹çš„ä¸»æœºåï¼Œè¯·åœ¨/etc/hostsé…ç½®æ–‡ä»¶ä¸­è¿›è¡Œä¸»æœºåæ˜ å°„è®¾ç½®
  - hostname: k8s-master03
    # masterèŠ‚ç‚¹çš„IPåœ°å€
    ipAddress: 192.168.0.103
    # masterèŠ‚ç‚¹äº’è®¿ä½¿ç”¨çš„ç½‘å¡åå­—ï¼Œç”¨äºkeepalivedç½‘å¡ç»‘å®š
    networkInterface: eth0
    # keepalivedé€‰ä¸¾ä¼˜å…ˆçº§ï¼Œæ•°å€¼è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜ï¼Œå„ä¸ªmasterèŠ‚ç‚¹çš„ä¼˜å…ˆçº§ä¸èƒ½ä¸€æ ·
    keepalivedPriority: 100

# é€šè¿‡dorycliåˆ›å»ºå¯ç”¨è´Ÿè½½å‡è¡¡å™¨é…ç½®ä¿¡æ¯ï¼Œå¹¶ä¸”æŠŠç”Ÿæˆçš„é…ç½®è¾“å‡ºåˆ°å½“å‰ç›®å½•
# æ‰§è¡Œå‘½ååï¼Œä¼šè¾“å‡ºç”Ÿæˆçš„æ–‡ä»¶è¯´æ˜ï¼Œä»¥åŠå¯åŠ¨é…ç½®æ–‡ä»¶è¯´æ˜
dorycli install ha script -o . -f kubeadm-ha.yaml --language zh

# æŸ¥çœ‹dorycliç”Ÿæˆçš„kubeadm-config.yamlé…ç½®æ–‡ä»¶ï¼Œè¯¥é…ç½®æ–‡ä»¶ç”¨äºkubeadm initåˆå§‹åŒ–kubernetesé›†ç¾¤ç”¨é€”
# æœ¬ä¾‹å­ç”Ÿæˆçš„é…ç½®å¦‚ä¸‹:
cat kubeadm-config.yaml
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v1.28.0
imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers
apiServer:
  certSANs:
    - "k8s-vip"
    - "192.168.0.100"
    - "k8s-master01"
    - "192.168.0.101"
    - "k8s-master02"
    - "192.168.0.102"
    - "k8s-master03"
    - "192.168.0.103"
controlPlaneEndpoint: "192.168.0.100:16443"
networking:
  podSubnet: "10.244.0.0/24"
  serviceSubnet: "10.96.0.0/16"
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock

# è®¾ç½®masterèŠ‚ç‚¹çš„kubernetesé«˜å¯ç”¨è´Ÿè½½å‡è¡¡å™¨çš„æ–‡ä»¶è·¯å¾„
export LB_DIR=/data/k8s-lb

# æŠŠé«˜å¯ç”¨è´Ÿè½½å‡è¡¡å™¨çš„æ–‡ä»¶å¤åˆ¶åˆ°k8s-master01
ssh k8s-master01 mkdir -p ${LB_DIR}
scp -r k8s-master01/nginx-lb k8s-master01/keepalived root@k8s-master01:${LB_DIR}

# åœ¨ k8s-master01 èŠ‚ç‚¹ä¸Šå¯åŠ¨é«˜å¯ç”¨è´Ÿè½½å‡è¡¡å™¨
ssh k8s-master01 "cd ${LB_DIR}/keepalived/ && docker-compose stop && docker-compose rm -f && docker-compose up -d"
ssh k8s-master01 "cd ${LB_DIR}/nginx-lb/ && docker-compose stop && docker-compose rm -f && docker-compose up -d"

# æŠŠé«˜å¯ç”¨è´Ÿè½½å‡è¡¡å™¨çš„æ–‡ä»¶å¤åˆ¶åˆ°k8s-master02
ssh k8s-master02 mkdir -p ${LB_DIR}
scp -r k8s-master02/nginx-lb k8s-master02/keepalived root@k8s-master02:${LB_DIR}

# åœ¨ k8s-master02 èŠ‚ç‚¹ä¸Šå¯åŠ¨é«˜å¯ç”¨è´Ÿè½½å‡è¡¡å™¨
ssh k8s-master02 "cd ${LB_DIR}/keepalived/ && docker-compose stop && docker-compose rm -f && docker-compose up -d"
ssh k8s-master02 "cd ${LB_DIR}/nginx-lb/ && docker-compose stop && docker-compose rm -f && docker-compose up -d"

# æŠŠé«˜å¯ç”¨è´Ÿè½½å‡è¡¡å™¨çš„æ–‡ä»¶å¤åˆ¶åˆ°k8s-master03
ssh k8s-master03 mkdir -p ${LB_DIR}
scp -r k8s-master03/nginx-lb k8s-master03/keepalived root@k8s-master03:${LB_DIR}

# åœ¨ k8s-master03 èŠ‚ç‚¹ä¸Šå¯åŠ¨é«˜å¯ç”¨è´Ÿè½½å‡è¡¡å™¨
ssh k8s-master03 "cd ${LB_DIR}/keepalived/ && docker-compose stop && docker-compose rm -f && docker-compose up -d"
ssh k8s-master03 "cd ${LB_DIR}/nginx-lb/ && docker-compose stop && docker-compose rm -f && docker-compose up -d"

# åœ¨å„ä¸ªmasterèŠ‚ç‚¹ä¸Šæ£€éªŒæµ®åŠ¨IPæ˜¯å¦å·²ç»åˆ›å»ºï¼Œæ­£å¸¸æƒ…å†µä¸‹æµ®åŠ¨IPç»‘å®šåœ¨ k8s-master01 ä¸Š
ip address
```

- åˆå§‹åŒ–é«˜å¯ç”¨kubernetesé›†ç¾¤

```bash
# åœ¨k8s-master01ä¸Šä½¿ç”¨kubeadm-config.yamlé…ç½®æ–‡ä»¶åˆå§‹åŒ–é«˜å¯ç”¨é›†ç¾¤
kubeadm init --config=kubeadm-config.yaml --upload-certs
# kubeadm initå‘½ä»¤å°†ä¼šè¾“å‡ºä»¥ä¸‹æç¤ºï¼Œä½¿ç”¨è¯¥æç¤ºåœ¨å…¶ä»–masterèŠ‚ç‚¹æ‰§è¡Œjoinæ“ä½œ
You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 192.168.0.100:16443 --token tgszyf.c9dicrflqy85juaf \
    --discovery-token-ca-cert-hash sha256:xxx \
    --control-plane --certificate-key xxx

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.0.100:16443 --token tgszyf.c9dicrflqy85juaf \
    --discovery-token-ca-cert-hash sha256:xxx 


  kubeadm join 192.168.0.100:16443 --token tgszyf.c9dicrflqy85juaf \
    --discovery-token-ca-cert-hash sha256:xxx \
    --control-plane --certificate-key xxx

# åœ¨k8s-master02 å’Œ k8s-master03èŠ‚ç‚¹ä¸Šæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼ŒæŠŠk8s-master02 å’Œ k8s-master03åŠ å…¥åˆ°é«˜å¯ç”¨kubernetesé›†ç¾¤
# è®°ä½kubeadm joinå‘½ä»¤éœ€è¦è®¾ç½®--cri-socket unix:///var/run/cri-dockerd.sock
kubeadm join 192.168.0.100:16443 --token tgszyf.c9dicrflqy85juaf \
        --discovery-token-ca-cert-hash sha256:xxx \
        --control-plane --certificate-key xxx --cri-socket unix:///var/run/cri-dockerd.sock

# åœ¨æ‰€æœ‰masterèŠ‚ç‚¹ä¸Šè®¾ç½®kubectlè®¿é—®kubernetesé›†ç¾¤
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

# åœ¨æ‰€æœ‰masterèŠ‚ç‚¹ä¸Šè®¾ç½®kubectlçš„è‡ªåŠ¨å®Œæˆï¼Œå¯ä»¥é€šè¿‡é”®ç›˜TABé”®è‡ªåŠ¨è¡¥å…¨å­å‘½ä»¤å’Œå‚æ•°
kubectl completion -h
kubectl completion bash > ~/.kube/completion.bash.inc
printf "
# Kubectl shell completion
source '$HOME/.kube/completion.bash.inc'
" >> $HOME/.bash_profile
source $HOME/.bash_profile

# åœ¨k8s-master01èŠ‚ç‚¹ä¸Šå®‰è£…ciliumç½‘ç»œç»„ä»¶
wget https://github.com/cilium/cilium-cli/releases/download/v0.15.6/cilium-linux-amd64.tar.gz
tar zxvf cilium-linux-amd64.tar.gz 
mv cilium /usr/local/bin/
cilium install --version 1.14.0 --set cni.chainingMode=portmap

# è®¾ç½®æ‰€æœ‰masterå…è®¸è°ƒåº¦pod
kubectl taint nodes --all node-role.kubernetes.io/control-plane-

# æ£€æŸ¥æ‰€æœ‰podçŠ¶æ€æ˜¯å¦æ­£å¸¸
kubectl get pods -A -o wide
NAMESPACE              NAME                                         READY   STATUS    RESTARTS      AGE     IP              NODE           NOMINATED NODE   READINESS GATES
kube-system            cilium-mwvsr                                 1/1     Running   0             21m     192.168.0.102   k8s-master02   <none>           <none>
kube-system            cilium-operator-b4dfbf784-zgr7v              1/1     Running   0             21m     192.168.0.102   k8s-master02   <none>           <none>
kube-system            cilium-v27l2                                 1/1     Running   0             21m     192.168.0.103   k8s-master03   <none>           <none>
kube-system            cilium-zbcdj                                 1/1     Running   0             21m     192.168.0.101   k8s-master01   <none>           <none>
kube-system            coredns-6554b8b87f-kp7tn                     1/1     Running   0             30m     10.0.2.231      k8s-master03   <none>           <none>
kube-system            coredns-6554b8b87f-zlhgx                     1/1     Running   0             30m     10.0.2.197      k8s-master03   <none>           <none>
kube-system            etcd-k8s-master01                            1/1     Running   0             30m     192.168.0.101   k8s-master01   <none>           <none>
kube-system            etcd-k8s-master02                            1/1     Running   0             26m     192.168.0.102   k8s-master02   <none>           <none>
kube-system            etcd-k8s-master03                            1/1     Running   0             25m     192.168.0.103   k8s-master03   <none>           <none>
kube-system            kube-apiserver-k8s-master01                  1/1     Running   0             30m     192.168.0.101   k8s-master01   <none>           <none>
kube-system            kube-apiserver-k8s-master02                  1/1     Running   0             26m     192.168.0.102   k8s-master02   <none>           <none>
kube-system            kube-apiserver-k8s-master03                  1/1     Running   1 (25m ago)   25m     192.168.0.103   k8s-master03   <none>           <none>
kube-system            kube-controller-manager-k8s-master01         1/1     Running   1 (26m ago)   30m     192.168.0.101   k8s-master01   <none>           <none>
kube-system            kube-controller-manager-k8s-master02         1/1     Running   0             26m     192.168.0.102   k8s-master02   <none>           <none>
kube-system            kube-controller-manager-k8s-master03         1/1     Running   0             24m     192.168.0.103   k8s-master03   <none>           <none>
kube-system            kube-proxy-gr2pt                             1/1     Running   0             26m     192.168.0.102   k8s-master02   <none>           <none>
kube-system            kube-proxy-rkb9b                             1/1     Running   0             30m     192.168.0.101   k8s-master01   <none>           <none>
kube-system            kube-proxy-rvmv4                             1/1     Running   0             25m     192.168.0.103   k8s-master03   <none>           <none>
kube-system            kube-scheduler-k8s-master01                  1/1     Running   1 (26m ago)   30m     192.168.0.101   k8s-master01   <none>           <none>
kube-system            kube-scheduler-k8s-master02                  1/1     Running   0             26m     192.168.0.102   k8s-master02   <none>           <none>
kube-system            kube-scheduler-k8s-master03                  1/1     Running   0             23m     192.168.0.103   k8s-master03   <none>           <none>

# æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€æ˜¯å¦æ­£å¸¸
kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
k8s-master01   Ready    control-plane   31m   v1.28.0
k8s-master02   Ready    control-plane   27m   v1.28.0
k8s-master03   Ready    control-plane   26m   v1.28.0

# æµ‹è¯•éƒ¨ç½²åº”ç”¨åˆ°kubernetesé›†ç¾¤
# éƒ¨ç½²ä¸€ä¸ªnginxåº”ç”¨ï¼Œå¹¶æš´éœ²åˆ°nodePort31000
kubectl run nginx --image=nginx:1.23.1-alpine --image-pull-policy=IfNotPresent --port=80 -l=app=nginx
kubectl create service nodeport nginx --tcp=80:80 --node-port=31000
curl k8s-vip:31000
```

## [å¯é€‰] å®‰è£…ç®¡ç†ç•Œé¢ kubernetes-dashboard

- ä¸ºäº†ç®¡ç†kubernetesä¸­éƒ¨ç½²çš„åº”ç”¨ï¼Œæ¨èä½¿ç”¨`kubernetes-dashboard`
- è¦äº†è§£æ›´å¤šï¼Œè¯·é˜…è¯»å®˜æ–¹ä»£ç ä»“åº“README.mdæ–‡æ¡£: [kubernetes-dashboard](https://github.com/kubernetes/dashboard)

- å®‰è£…:
```shell script
# å®‰è£… kubernetes-dashboard
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.1/aio/deploy/recommended.yaml

# è°ƒæ•´kubernetes-dashboardæœåŠ¡ä½¿ç”¨nodePortæš´éœ²ç«¯å£
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  ports:
  - port: 443
    protocol: TCP
    targetPort: 8443
    nodePort: 30000
  selector:
    k8s-app: kubernetes-dashboard
  type: NodePort
EOF

# åˆ›å»ºç®¡ç†å‘˜serviceaccount
kubectl create serviceaccount -n kube-system admin-user --dry-run=client -o yaml | kubectl apply -f -

# åˆ›å»ºç®¡ç†å‘˜clusterrolebinding
kubectl create clusterrolebinding admin-user --clusterrole=cluster-admin --serviceaccount=kube-system:admin-user --dry-run=client -o yaml | kubectl apply -f -

# æ‰‹åŠ¨åˆ›å»ºserviceaccountçš„secret
cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: admin-user-secret
  namespace: kube-system
  annotations:
    kubernetes.io/service-account.name: admin-user
type: kubernetes.io/service-account-token
EOF

# è·å–kubernetesç®¡ç†token
kubectl -n kube-system get secret admin-user-secret -o jsonpath='{ .data.token }' | base64 -d

# ä½¿ç”¨æµè§ˆå™¨è®¿é—®kubernetes-dashboard: https://k8s-vip:30000
# ä½¿ç”¨kubernetesç®¡ç†tokenç™»å½•kubernetes-dashboard
```

## [å¯é€‰] å®‰è£…ingressæ§åˆ¶å™¨ traefik

- è¦ä½¿ç”¨kubernetesçš„[ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)åŠŸèƒ½ï¼Œå¿…é¡»å®‰è£…ingress controllerï¼Œæ¨èä½¿ç”¨`traefik`
- è¦äº†è§£æ›´å¤šï¼Œè¯·é˜…è¯»å®˜æ–¹ç½‘ç«™æ–‡æ¡£: [traefik](https://doc.traefik.io/traefik/)

- åœ¨kubernetesæ‰€æœ‰masterèŠ‚ç‚¹éƒ¨ç½²traefik: 
```shell script
# æ‹‰å– traefik helm repo
helm repo add traefik https://helm.traefik.io/traefik
helm fetch traefik/traefik --untar

# ä»¥daemonsetæ–¹å¼éƒ¨ç½²traefik
cat << EOF > traefik.yaml
deployment:
  kind: DaemonSet
image:
  name: traefik
  tag: v2.6.3
pilot:
  enabled: true
experimental:
  plugins:
    enabled: true
ports:
  web:
    hostPort: 80
  websecure:
    hostPort: 443
service:
  type: ClusterIP
EOF

# å®‰è£…traefik
kubectl create namespace traefik --dry-run=client -o yaml | kubectl apply -f -
helm install -n traefik traefik traefik/ -f traefik.yaml

# æ£€æŸ¥å®‰è£…æƒ…å†µ
helm -n traefik list
kubectl -n traefik get pods -o wide
kubectl -n traefik get services -o wide

# æ£€éªŒtraefikå®‰è£…æ˜¯å¦æˆåŠŸï¼Œå¦‚æœè¾“å‡º 404 page not found è¡¨ç¤ºæˆåŠŸ
curl k8s-vip
curl -k https://k8s-vip
```

## [å¯é€‰] å®‰è£…æ€§èƒ½æ•°æ®é‡‡é›†å·¥å…· metrics-server

- ä¸ºäº†ä½¿ç”¨kubernetesçš„æ°´å¹³æ‰©å±•ç¼©å®¹åŠŸèƒ½[horizontal pod autoscale](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)ï¼Œå¿…é¡»å®‰è£…`metrics-server`
- è¦äº†è§£æ›´å¤šï¼Œè¯·é˜…è¯»å®˜æ–¹ä»£ç ä»“åº“README.mdæ–‡æ¡£: [metrics-server](https://github.com/kubernetes-sigs/metrics-server)

```shell script
# æ‹‰å–é•œåƒ
docker pull registry.aliyuncs.com/google_containers/metrics-server:v0.6.1
docker tag registry.aliyuncs.com/google_containers/metrics-server:v0.6.1 k8s.gcr.io/metrics-server/metrics-server:v0.6.1

# è·å–metrics-serverå®‰è£…yaml
curl -O -L https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.1/components.yaml
# æ·»åŠ --kubelet-insecure-tlså‚æ•°
sed -i 's/- args:/- args:\n        - --kubelet-insecure-tls/g' components.yaml
# å®‰è£…metrics-server
kubectl apply -f components.yaml

# ç­‰å¾…metrics-serveræ­£å¸¸
kubectl -n kube-system get pods -l=k8s-app=metrics-server

# æŸ¥çœ‹èŠ‚ç‚¹çš„metrics
kubectl top nodes
NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s-master01   146m         7%     2284Mi          59%       
k8s-master02   123m         6%     2283Mi          59%       
k8s-master03   114m         5%     2180Mi          57%       
```

- å®‰è£…metrics-serveråkubernetes-dashboardä¹Ÿå¯ä»¥æ˜¾ç¤ºæ€§èƒ½æ•°æ®

![](images/kubernetes-dashboard.png)

## [å¯é€‰] å®‰è£…æœåŠ¡ç½‘æ ¼ istio

- è¦ä½¿ç”¨æœåŠ¡ç½‘æ ¼çš„æ··åˆç°åº¦å‘å¸ƒèƒ½åŠ›ï¼Œéœ€è¦éƒ¨ç½²istioæœåŠ¡ç½‘æ ¼
- è¦äº†è§£æ›´å¤šï¼Œè¯·é˜…è¯»istioå®˜ç½‘æ–‡æ¡£: [istio.io](https://istio.io/latest/docs/)

```shell script
# å®‰è£…istioctlï¼Œå®¢æˆ·ç«¯ä¸‹è½½åœ°å€ https://github.com/istio/istio/releases/tag/1.18.2

# ä¸‹è½½å¹¶å®‰è£…istioctl
wget https://github.com/istio/istio/releases/download/1.18.2/istioctl-1.18.2-linux-amd64.tar.gz
tar zxvf istioctl-1.18.2-linux-amd64.tar.gz
mv istioctl /usr/bin/

# ç¡®è®¤istioctlç‰ˆæœ¬
istioctl version

# ä½¿ç”¨istioctléƒ¨ç½²istioåˆ°kubernetes
istioctl install --set profile=demo \
--set values.gateways.istio-ingressgateway.type=ClusterIP \
--set values.global.imagePullPolicy=IfNotPresent \
--set values.global.proxy_init.resources.limits.cpu=100m \
--set values.global.proxy_init.resources.limits.memory=100Mi \
--set values.global.proxy.resources.limits.cpu=100m \
--set values.global.proxy.resources.limits.memory=100Mi

# æ£€æŸ¥istioéƒ¨ç½²æƒ…å†µ
kubectl -n istio-system get pods,svc
```

## [å¯é€‰] åº”ç”¨ä¸Šäº‘å¼•æ“ Dory-Engine

[ğŸš€ğŸš€ğŸš€ Dory-Engineå¹³å°å·¥ç¨‹æœ€ä½³å®è·µ (https://www.bilibili.com/video/BV1oM4y117Pj/)](https://www.bilibili.com/video/BV1oM4y117Pj/)

![](images/what-is-dory.png)

- `Dory-Engine` æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„åº”ç”¨ä¸Šäº‘å¼•æ“ï¼Œå¼€å‘äººå‘˜ä¸ç”¨å­¦ã€ä¸ç”¨å†™ã€ä¸ç”¨é…å°±å¯ä»¥è‡ªè¡ŒæŠŠè‡ªå·±ç¼–å†™çš„ç¨‹åºä»æºä»£ç ï¼Œç¼–è¯‘ã€æ‰“åŒ…ã€éƒ¨ç½²åˆ°å„ç±»k8sç¯å¢ƒæˆ–è€…ä¸»æœºç¯å¢ƒä¸­ã€‚

1. ä¸ç”¨å­¦: ä¸éœ€è¦å­¦ä¹ å¦‚ä½•ç¼–å†™å¤æ‚çš„ä¸Šäº‘è„šæœ¬å’Œå¦‚ä½•éƒ¨ç½²åº”ç”¨åˆ°k8sï¼Œæ‰€æœ‰é…ç½®éƒ½æ‰€è§å³æ‰€å¾—ä¸€çœ‹å°±æ‡‚
2. ä¸ç”¨å†™: ä¸éœ€è¦ç¼–å†™å¤æ‚çš„æ„å»ºã€æ‰“åŒ…ã€éƒ¨ç½²çš„ä¸Šäº‘è„šæœ¬ï¼Œä¹Ÿä¸éœ€è¦ç¼–å†™å¤æ‚çš„k8såº”ç”¨éƒ¨ç½²æ–‡ä»¶ï¼Œåªéœ€è¦å‡ é¡¹ç®€å•çš„é…ç½®å°±å¯ä»¥è®¾ç½®å¥½è‡ªå·±çš„ä¸Šäº‘æµæ°´çº¿
3. ä¸ç”¨é…: ä¸éœ€è¦é…ç½®å„ä¸ªDevOpså·¥å…·é“¾å’Œk8sç¯å¢ƒå¦‚ä½•äº’ç›¸é…åˆå®Œæˆåº”ç”¨ä¸Šäº‘ï¼Œé¡¹ç›®ä¸€å¼€é€šæ‰€æœ‰å·¥å…·é“¾å’Œç¯å¢ƒè‡ªåŠ¨å®Œæˆé…ç½®

- å®‰è£…æŒ‡å¼•å‚è§: [https://github.com/dory-engine/dorycli](https://github.com/dory-engine/dorycli)

[ğŸš€ğŸš€ğŸš€ ä½¿ç”¨dorycliå®‰è£…éƒ¨ç½²Dory-Engine (https://www.bilibili.com/video/BV1x94y167T5/)](https://www.bilibili.com/video/BV1x94y167T5/)
