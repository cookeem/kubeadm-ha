# kubeadm-highavailiability - åŸºäºkubeadmçš„kubernetesé«˜å¯ç”¨é›†ç¾¤éƒ¨ç½²ï¼Œæ”¯æŒv1.7.xç‰ˆæœ¬ä»¥åŠv1.6.xç‰ˆæœ¬

![k8s logo](images/Kubernetes.png)

- [ä¸­æ–‡æ–‡æ¡£(for v1.7.xç‰ˆæœ¬)](README_CN.md)
- [English document(for v1.7.x version)](README.md)
- [ä¸­æ–‡æ–‡æ¡£(for v1.6.xç‰ˆæœ¬)](README_v1.6.x_CN.md)
- [English document(for v1.6.x version)](README_v1.6.x.md)

---

- [GitHubé¡¹ç›®åœ°å€](https://github.com/cookeem/kubeadm-ha/)
- [OSChinaé¡¹ç›®åœ°å€](https://git.oschina.net/cookeem/kubeadm-ha/)

---

- è¯¥æŒ‡å¼•é€‚ç”¨äºv1.7.xç‰ˆæœ¬çš„kubernetesé›†ç¾¤

### ç›®å½•

1. [éƒ¨ç½²æ¶æ„](#éƒ¨ç½²æ¶æ„)
    1. [æ¦‚è¦éƒ¨ç½²æ¶æ„](#æ¦‚è¦éƒ¨ç½²æ¶æ„)
    1. [è¯¦ç»†éƒ¨ç½²æ¶æ„](#è¯¦ç»†éƒ¨ç½²æ¶æ„)
    1. [ä¸»æœºèŠ‚ç‚¹æ¸…å•](#ä¸»æœºèŠ‚ç‚¹æ¸…å•)
1. [å®‰è£…å‰å‡†å¤‡](#å®‰è£…å‰å‡†å¤‡)
    1. [ç‰ˆæœ¬ä¿¡æ¯](#ç‰ˆæœ¬ä¿¡æ¯)
    1. [æ‰€éœ€dockeré•œåƒ](#æ‰€éœ€dockeré•œåƒ)
    1. [ç³»ç»Ÿè®¾ç½®](#ç³»ç»Ÿè®¾ç½®)
1. [kuberneteså®‰è£…](#kuberneteså®‰è£…)
    1. [kubernetesç›¸å…³æœåŠ¡å®‰è£…](#kubernetesç›¸å…³æœåŠ¡å®‰è£…)
    1. [dockeré•œåƒå¯¼å…¥](#dockeré•œåƒå¯¼å…¥)
1. [ç¬¬ä¸€å°masteråˆå§‹åŒ–](#ç¬¬ä¸€å°masteråˆå§‹åŒ–)
    1. [ç‹¬ç«‹etcdé›†ç¾¤éƒ¨ç½²](#ç‹¬ç«‹etcdé›†ç¾¤éƒ¨ç½²)
    1. [kubeadmåˆå§‹åŒ–](#kubeadmåˆå§‹åŒ–)
    1. [flannelç½‘ç»œç»„ä»¶å®‰è£…](#flannelç½‘ç»œç»„ä»¶å®‰è£…)
    1. [dashboardç»„ä»¶å®‰è£…](#dashboardç»„ä»¶å®‰è£…)
    1. [heapsterç»„ä»¶å®‰è£…](#heapsterç»„ä»¶å®‰è£…)
1. [masteré›†ç¾¤é«˜å¯ç”¨è®¾ç½®](#masteré›†ç¾¤é«˜å¯ç”¨è®¾ç½®)
    1. [å¤åˆ¶é…ç½®](#å¤åˆ¶é…ç½®)
    1. [ä¿®æ”¹é…ç½®](#ä¿®æ”¹é…ç½®)
    1. [éªŒè¯é«˜å¯ç”¨å®‰è£…](#éªŒè¯é«˜å¯ç”¨å®‰è£…)
    1. [keepalivedå®‰è£…é…ç½®](#keepalivedå®‰è£…é…ç½®)
    1. [nginxè´Ÿè½½å‡è¡¡é…ç½®](#nginxè´Ÿè½½å‡è¡¡é…ç½®)
    1. [kube-proxyé…ç½®](#kube-proxyé…ç½®)
    1. [éªŒè¯masteré›†ç¾¤é«˜å¯ç”¨](#éªŒè¯masteré›†ç¾¤é«˜å¯ç”¨)
1. [nodeèŠ‚ç‚¹åŠ å…¥é«˜å¯ç”¨é›†ç¾¤è®¾ç½®](#nodeèŠ‚ç‚¹åŠ å…¥é«˜å¯ç”¨é›†ç¾¤è®¾ç½®)
    1. [kubeadmåŠ å…¥é«˜å¯ç”¨é›†ç¾¤](#kubeadmåŠ å…¥é«˜å¯ç”¨é›†ç¾¤)
    1. [éƒ¨ç½²åº”ç”¨éªŒè¯é›†ç¾¤](#éƒ¨ç½²åº”ç”¨éªŒè¯é›†ç¾¤)
    

### éƒ¨ç½²æ¶æ„

#### æ¦‚è¦éƒ¨ç½²æ¶æ„

![ha logo](images/ha.png)

* kubernetesé«˜å¯ç”¨çš„æ ¸å¿ƒæ¶æ„æ˜¯masterçš„é«˜å¯ç”¨ï¼Œkubectlã€å®¢æˆ·ç«¯ä»¥åŠnodesè®¿é—®load balancerå®ç°é«˜å¯ç”¨ã€‚

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### è¯¦ç»†éƒ¨ç½²æ¶æ„

![k8s ha](images/k8s-ha.png)

* kubernetesç»„ä»¶è¯´æ˜

> kube-apiserverï¼šé›†ç¾¤æ ¸å¿ƒï¼Œé›†ç¾¤APIæ¥å£ã€é›†ç¾¤å„ä¸ªç»„ä»¶é€šä¿¡çš„ä¸­æ¢ï¼›é›†ç¾¤å®‰å…¨æ§åˆ¶ï¼›

> etcdï¼šé›†ç¾¤çš„æ•°æ®ä¸­å¿ƒï¼Œç”¨äºå­˜æ”¾é›†ç¾¤çš„é…ç½®ä»¥åŠçŠ¶æ€ä¿¡æ¯ï¼Œéå¸¸é‡è¦ï¼Œå¦‚æœæ•°æ®ä¸¢å¤±é‚£ä¹ˆé›†ç¾¤å°†æ— æ³•æ¢å¤ï¼›å› æ­¤é«˜å¯ç”¨é›†ç¾¤éƒ¨ç½²é¦–å…ˆå°±æ˜¯etcdæ˜¯é«˜å¯ç”¨é›†ç¾¤ï¼›

> kube-schedulerï¼šé›†ç¾¤Podçš„è°ƒåº¦ä¸­å¿ƒï¼›é»˜è®¤kubeadmå®‰è£…æƒ…å†µä¸‹--leader-electå‚æ•°å·²ç»è®¾ç½®ä¸ºtrueï¼Œä¿è¯masteré›†ç¾¤ä¸­åªæœ‰ä¸€ä¸ªkube-schedulerå¤„äºæ´»è·ƒçŠ¶æ€ï¼›

> kube-controller-managerï¼šé›†ç¾¤çŠ¶æ€ç®¡ç†å™¨ï¼Œå½“é›†ç¾¤çŠ¶æ€ä¸æœŸæœ›ä¸åŒæ—¶ï¼Œkcmä¼šåŠªåŠ›è®©é›†ç¾¤æ¢å¤æœŸæœ›çŠ¶æ€ï¼Œæ¯”å¦‚ï¼šå½“ä¸€ä¸ªpodæ­»æ‰ï¼Œkcmä¼šåŠªåŠ›æ–°å»ºä¸€ä¸ªpodæ¥æ¢å¤å¯¹åº”replicas setæœŸæœ›çš„çŠ¶æ€ï¼›é»˜è®¤kubeadmå®‰è£…æƒ…å†µä¸‹--leader-electå‚æ•°å·²ç»è®¾ç½®ä¸ºtrueï¼Œä¿è¯masteré›†ç¾¤ä¸­åªæœ‰ä¸€ä¸ªkube-controller-managerå¤„äºæ´»è·ƒçŠ¶æ€ï¼›

> kubelet: kubernetes node agentï¼Œè´Ÿè´£ä¸nodeä¸Šçš„docker engineæ‰“äº¤é“ï¼›

> kube-proxy: æ¯ä¸ªnodeä¸Šä¸€ä¸ªï¼Œè´Ÿè´£service vipåˆ°endpoint podçš„æµé‡è½¬å‘ï¼Œå½“å‰ä¸»è¦é€šè¿‡è®¾ç½®iptablesè§„åˆ™å®ç°ã€‚

* è´Ÿè½½å‡è¡¡

> keepalivedé›†ç¾¤è®¾ç½®ä¸€ä¸ªè™šæ‹Ÿipåœ°å€ï¼Œè™šæ‹Ÿipåœ°å€æŒ‡å‘k8s-master1ã€k8s-master2ã€k8s-master3ã€‚

> nginxç”¨äºk8s-master1ã€k8s-master2ã€k8s-master3çš„apiserverçš„è´Ÿè½½å‡è¡¡ã€‚å¤–éƒ¨kubectlä»¥åŠnodesè®¿é—®apiserverçš„æ—¶å€™å°±å¯ä»¥ç”¨è¿‡keepalivedçš„è™šæ‹Ÿip(192.168.60.80)ä»¥åŠnginxç«¯å£(8443)è®¿é—®masteré›†ç¾¤çš„apiserverã€‚

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### ä¸»æœºèŠ‚ç‚¹æ¸…å•

 ä¸»æœºå | IPåœ°å€ | è¯´æ˜ | ç»„ä»¶ 
 :--- | :--- | :--- | :---
 k8s-master1 | 192.168.60.71 | masterèŠ‚ç‚¹1 | keepalivedã€nginxã€etcdã€kubeletã€kube-apiserverã€kube-schedulerã€kube-proxyã€kube-dashboardã€heapster
 k8s-master2 | 192.168.60.72 | masterèŠ‚ç‚¹2 | keepalivedã€nginxã€etcdã€kubeletã€kube-apiserverã€kube-schedulerã€kube-proxyã€kube-dashboardã€heapster
 k8s-master3 | 192.168.60.73 | masterèŠ‚ç‚¹3 | keepalivedã€nginxã€etcdã€kubeletã€kube-apiserverã€kube-schedulerã€kube-proxyã€kube-dashboardã€heapster
 æ—  | 192.168.60.80 | keepalivedè™šæ‹ŸIP | æ— 
 k8s-node1 ~ 8 | 192.168.60.81 ~ 88 | 8ä¸ªnodeèŠ‚ç‚¹ | kubeletã€kube-proxy

---
[è¿”å›ç›®å½•](#ç›®å½•)

### å®‰è£…å‰å‡†å¤‡

#### ç‰ˆæœ¬ä¿¡æ¯

* Linuxç‰ˆæœ¬ï¼šCentOS 7.3.1611

```
cat /etc/redhat-release 
CentOS Linux release 7.3.1611 (Core) 
```

* dockerç‰ˆæœ¬ï¼š1.12.6

```
$ docker version
Client:
 Version:      1.12.6
 API version:  1.24
 Go version:   go1.6.4
 Git commit:   78d1802
 Built:        Tue Jan 10 20:20:01 2017
 OS/Arch:      linux/amd64

Server:
 Version:      1.12.6
 API version:  1.24
 Go version:   go1.6.4
 Git commit:   78d1802
 Built:        Tue Jan 10 20:20:01 2017
 OS/Arch:      linux/amd64
```

* kubeadmç‰ˆæœ¬ï¼šv1.7.0

```
$ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"7", GitVersion:"v1.7.0", GitCommit:"d3ada0119e776222f11ec7945e6d860061339aad", GitTreeState:"clean", BuildDate:"2017-06-29T22:55:19Z", GoVersion:"go1.8.3", Compiler:"gc", Platform:"linux/amd64"}
```

* kubeletç‰ˆæœ¬ï¼šv1.7.0

```
$ kubelet --version
Kubernetes v1.7.0
```

---

[è¿”å›ç›®å½•](#ç›®å½•)

#### æ‰€éœ€dockeré•œåƒ

* å›½å†…å¯ä»¥ä½¿ç”¨daocloudåŠ é€Ÿå™¨ä¸‹è½½ç›¸å…³é•œåƒï¼Œç„¶åé€šè¿‡docker saveã€docker loadæŠŠæœ¬åœ°ä¸‹è½½çš„é•œåƒæ”¾åˆ°kubernetesé›†ç¾¤çš„æ‰€åœ¨æœºå™¨ä¸Šï¼ŒdaocloudåŠ é€Ÿå™¨é“¾æ¥å¦‚ä¸‹ï¼š

[https://www.daocloud.io/mirror#accelerator-doc](https://www.daocloud.io/mirror#accelerator-doc)

* åœ¨æœ¬æœºMacOSXä¸Špullç›¸å…³dockeré•œåƒ

```
$ docker pull gcr.io/google_containers/kube-proxy-amd64:v1.7.0
$ docker pull gcr.io/google_containers/kube-apiserver-amd64:v1.7.0
$ docker pull gcr.io/google_containers/kube-controller-manager-amd64:v1.7.0
$ docker pull gcr.io/google_containers/kube-scheduler-amd64:v1.7.0
$ docker pull gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4
$ docker pull gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4
$ docker pull gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4
$ docker pull nginx:latest
$ docker pull gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.1
$ docker pull quay.io/coreos/flannel:v0.7.1-amd64
$ docker pull gcr.io/google_containers/heapster-amd64:v1.3.0
$ docker pull gcr.io/google_containers/etcd-amd64:3.0.17
$ docker pull gcr.io/google_containers/heapster-grafana-amd64:v4.0.2
$ docker pull gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1
$ docker pull gcr.io/google_containers/pause-amd64:3.0
```

* åœ¨æœ¬æœºMacOSXä¸Šè·å–ä»£ç ï¼Œå¹¶è¿›å…¥ä»£ç ç›®å½•

```
$ git clone https://github.com/cookeem/kubeadm-ha
$ cd kubeadm-ha
```

* åœ¨æœ¬æœºMacOSXä¸ŠæŠŠç›¸å…³dockeré•œåƒä¿å­˜æˆæ–‡ä»¶

```
$ mkdir -p docker-images
$ docker save -o docker-images/kube-proxy-amd64  gcr.io/google_containers/kube-proxy-amd64:v1.7.0
$ docker save -o docker-images/kube-apiserver-amd64  gcr.io/google_containers/kube-apiserver-amd64:v1.7.0
$ docker save -o docker-images/kube-controller-manager-amd64  gcr.io/google_containers/kube-controller-manager-amd64:v1.7.0
$ docker save -o docker-images/kube-scheduler-amd64  gcr.io/google_containers/kube-scheduler-amd64:v1.7.0
$ docker save -o docker-images/k8s-dns-sidecar-amd64  gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4
$ docker save -o docker-images/k8s-dns-kube-dns-amd64  gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4
$ docker save -o docker-images/k8s-dns-dnsmasq-nanny-amd64  gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4
$ docker save -o docker-images/heapster-grafana-amd64  gcr.io/google_containers/heapster-grafana-amd64:v4.2.0
$ docker save -o docker-images/nginx  nginx:latest
$ docker save -o docker-images/kubernetes-dashboard-amd64  gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.1
$ docker save -o docker-images/flannel  quay.io/coreos/flannel:v0.7.1-amd64
$ docker save -o docker-images/heapster-amd64  gcr.io/google_containers/heapster-amd64:v1.3.0
$ docker save -o docker-images/etcd-amd64  gcr.io/google_containers/etcd-amd64:3.0.17
$ docker save -o docker-images/heapster-grafana-amd64  gcr.io/google_containers/heapster-grafana-amd64:v4.0.2
$ docker save -o docker-images/heapster-influxdb-amd64  gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1
$ docker save -o docker-images/pause-amd64  gcr.io/google_containers/pause-amd64:3.0
```

* åœ¨æœ¬æœºMacOSXä¸ŠæŠŠä»£ç ä»¥åŠdockeré•œåƒå¤åˆ¶åˆ°æ‰€æœ‰èŠ‚ç‚¹ä¸Š

```
$ scp -r * root@k8s-master1:/root/kubeadm-ha
$ scp -r * root@k8s-master2:/root/kubeadm-ha
$ scp -r * root@k8s-master3:/root/kubeadm-ha
$ scp -r * root@k8s-node1:/root/kubeadm-ha
$ scp -r * root@k8s-node2:/root/kubeadm-ha
$ scp -r * root@k8s-node3:/root/kubeadm-ha
$ scp -r * root@k8s-node4:/root/kubeadm-ha
$ scp -r * root@k8s-node5:/root/kubeadm-ha
$ scp -r * root@k8s-node6:/root/kubeadm-ha
$ scp -r * root@k8s-node7:/root/kubeadm-ha
$ scp -r * root@k8s-node8:/root/kubeadm-ha
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### ç³»ç»Ÿè®¾ç½®

* ä»¥ä¸‹åœ¨kubernetesæ‰€æœ‰èŠ‚ç‚¹ä¸Šéƒ½æ˜¯ä½¿ç”¨rootç”¨æˆ·è¿›è¡Œæ“ä½œ

* åœ¨kubernetesæ‰€æœ‰èŠ‚ç‚¹ä¸Šå¢åŠ kubernetesä»“åº“ 

```
$ cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
```

* åœ¨kubernetesæ‰€æœ‰èŠ‚ç‚¹ä¸Šè¿›è¡Œç³»ç»Ÿæ›´æ–°

```
$ yum update -y
```

* åœ¨kubernetesæ‰€æœ‰èŠ‚ç‚¹ä¸Šå…³é—­é˜²ç«å¢™

```
$ systemctl disable firewalld && systemctl stop firewalld && systemctl status firewalld
```

* åœ¨kubernetesæ‰€æœ‰èŠ‚ç‚¹ä¸Šè®¾ç½®SELINUXä¸ºpermissiveæ¨¡å¼

```
$ vi /etc/selinux/config
SELINUX=permissive
```

* åœ¨kubernetesæ‰€æœ‰èŠ‚ç‚¹ä¸Šè®¾ç½®iptableså‚æ•°ï¼Œå¦åˆ™kubeadm initä¼šæç¤ºé”™è¯¯

```
$ vi /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
```

* åœ¨kubernetesæ‰€æœ‰èŠ‚ç‚¹ä¸Šé‡å¯ä¸»æœº

```
$ reboot
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

### kuberneteså®‰è£…

#### kubernetesç›¸å…³æœåŠ¡å®‰è£…

* åœ¨kubernetesæ‰€æœ‰èŠ‚ç‚¹ä¸ŠéªŒè¯SELINUXæ¨¡å¼ï¼Œå¿…é¡»ä¿è¯SELINUXä¸ºpermissiveæ¨¡å¼ï¼Œå¦åˆ™kuberneteså¯åŠ¨ä¼šå‡ºç°å„ç§å¼‚å¸¸

```
$ getenforce
Permissive
```

* åœ¨kubernetesæ‰€æœ‰èŠ‚ç‚¹ä¸Šå®‰è£…å¹¶å¯åŠ¨kubernetes 

```
$ yum search docker --showduplicates
$ yum install docker-1.12.6-16.el7.centos.x86_64

$ yum search kubelet --showduplicates
$ yum install kubelet-1.7.0-0.x86_64

$ yum search kubeadm --showduplicates
$ yum install kubeadm-1.7.0-0.x86_64

$ yum search kubernetes-cni --showduplicates
$ yum install kubernetes-cni-0.5.1-0.x86_64

$ systemctl enable docker && systemctl start docker
$ systemctl enable kubelet && systemctl start kubelet
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### dockeré•œåƒå¯¼å…¥

* åœ¨kubernetesæ‰€æœ‰èŠ‚ç‚¹ä¸Šå¯¼å…¥dockeré•œåƒ 

```
$ docker load -i /root/kubeadm-ha/docker-images/etcd-amd64
$ docker load -i /root/kubeadm-ha/docker-images/flannel
$ docker load -i /root/kubeadm-ha/docker-images/heapster-amd64
$ docker load -i /root/kubeadm-ha/docker-images/heapster-grafana-amd64
$ docker load -i /root/kubeadm-ha/docker-images/heapster-influxdb-amd64
$ docker load -i /root/kubeadm-ha/docker-images/k8s-dns-dnsmasq-nanny-amd64
$ docker load -i /root/kubeadm-ha/docker-images/k8s-dns-kube-dns-amd64
$ docker load -i /root/kubeadm-ha/docker-images/k8s-dns-sidecar-amd64
$ docker load -i /root/kubeadm-ha/docker-images/kube-apiserver-amd64
$ docker load -i /root/kubeadm-ha/docker-images/kube-controller-manager-amd64
$ docker load -i /root/kubeadm-ha/docker-images/kube-proxy-amd64
$ docker load -i /root/kubeadm-ha/docker-images/kubernetes-dashboard-amd64
$ docker load -i /root/kubeadm-ha/docker-images/kube-scheduler-amd64
$ docker load -i /root/kubeadm-ha/docker-images/pause-amd64
$ docker load -i /root/kubeadm-ha/docker-images/nginx

$ docker images
REPOSITORY                                               TAG                 IMAGE ID            CREATED             SIZE
gcr.io/google_containers/kube-proxy-amd64                v1.7.0              d2d44013d0f8        4 days ago          114.7 MB
gcr.io/google_containers/kube-apiserver-amd64            v1.7.0              f0d4b746fb2b        4 days ago          185.2 MB
gcr.io/google_containers/kube-controller-manager-amd64   v1.7.0              36bf73ed0632        4 days ago          137 MB
gcr.io/google_containers/kube-scheduler-amd64            v1.7.0              5c9a7f60a95c        4 days ago          77.16 MB
gcr.io/google_containers/k8s-dns-sidecar-amd64           1.14.4              38bac66034a6        7 days ago          41.81 MB
gcr.io/google_containers/k8s-dns-kube-dns-amd64          1.14.4              a8e00546bcf3        7 days ago          49.38 MB
gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64     1.14.4              f7f45b9cb733        7 days ago          41.41 MB
nginx                                                    latest              958a7ae9e569        4 weeks ago         109.4 MB
gcr.io/google_containers/kubernetes-dashboard-amd64      v1.6.1              71dfe833ce74        6 weeks ago         134.4 MB
quay.io/coreos/flannel                                   v0.7.1-amd64        cd4ae0be5e1b        10 weeks ago        77.76 MB
gcr.io/google_containers/heapster-amd64                  v1.3.0              f9d33bedfed3        3 months ago        68.11 MB
gcr.io/google_containers/etcd-amd64                      3.0.17              243830dae7dd        4 months ago        168.9 MB
gcr.io/google_containers/heapster-grafana-amd64          v4.0.2              a1956d2a1a16        5 months ago        131.5 MB
gcr.io/google_containers/heapster-influxdb-amd64         v1.1.1              d3fccbedd180        5 months ago        11.59 MB
gcr.io/google_containers/pause-amd64                     3.0                 99e59f495ffa        14 months ago       746.9 kB
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

### ç¬¬ä¸€å°masteråˆå§‹åŒ–

#### ç‹¬ç«‹etcdé›†ç¾¤éƒ¨ç½²

* åœ¨k8s-master1èŠ‚ç‚¹ä¸Šä»¥dockeræ–¹å¼å¯åŠ¨etcdé›†ç¾¤

```
$ docker stop etcd && docker rm etcd
$ rm -rf /var/lib/etcd-cluster
$ mkdir -p /var/lib/etcd-cluster
$ docker run -d \
--restart always \
-v /etc/ssl/certs:/etc/ssl/certs \
-v /var/lib/etcd-cluster:/var/lib/etcd \
-p 4001:4001 \
-p 2380:2380 \
-p 2379:2379 \
--name etcd \
gcr.io/google_containers/etcd-amd64:3.0.17 \
etcd --name=etcd0 \
--advertise-client-urls=http://192.168.60.71:2379,http://192.168.60.71:4001 \
--listen-client-urls=http://0.0.0.0:2379,http://0.0.0.0:4001 \
--initial-advertise-peer-urls=http://192.168.60.71:2380 \
--listen-peer-urls=http://0.0.0.0:2380 \
--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \
--initial-cluster=etcd0=http://192.168.60.71:2380,etcd1=http://192.168.60.72:2380,etcd2=http://192.168.60.73:2380 \
--initial-cluster-state=new \
--auto-tls \
--peer-auto-tls \
--data-dir=/var/lib/etcd
```

* åœ¨k8s-master2èŠ‚ç‚¹ä¸Šä»¥dockeræ–¹å¼å¯åŠ¨etcdé›†ç¾¤

```
$ docker stop etcd && docker rm etcd
$ rm -rf /var/lib/etcd-cluster
$ mkdir -p /var/lib/etcd-cluster
$ docker run -d \
--restart always \
-v /etc/ssl/certs:/etc/ssl/certs \
-v /var/lib/etcd-cluster:/var/lib/etcd \
-p 4001:4001 \
-p 2380:2380 \
-p 2379:2379 \
--name etcd \
gcr.io/google_containers/etcd-amd64:3.0.17 \
etcd --name=etcd1 \
--advertise-client-urls=http://192.168.60.72:2379,http://192.168.60.72:4001 \
--listen-client-urls=http://0.0.0.0:2379,http://0.0.0.0:4001 \
--initial-advertise-peer-urls=http://192.168.60.72:2380 \
--listen-peer-urls=http://0.0.0.0:2380 \
--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \
--initial-cluster=etcd0=http://192.168.60.71:2380,etcd1=http://192.168.60.72:2380,etcd2=http://192.168.60.73:2380 \
--initial-cluster-state=new \
--auto-tls \
--peer-auto-tls \
--data-dir=/var/lib/etcd
```

* åœ¨k8s-master3èŠ‚ç‚¹ä¸Šä»¥dockeræ–¹å¼å¯åŠ¨etcdé›†ç¾¤

```
$ docker stop etcd && docker rm etcd
$ rm -rf /var/lib/etcd-cluster
$ mkdir -p /var/lib/etcd-cluster
$ docker run -d \
--restart always \
-v /etc/ssl/certs:/etc/ssl/certs \
-v /var/lib/etcd-cluster:/var/lib/etcd \
-p 4001:4001 \
-p 2380:2380 \
-p 2379:2379 \
--name etcd \
gcr.io/google_containers/etcd-amd64:3.0.17 \
etcd --name=etcd2 \
--advertise-client-urls=http://192.168.60.73:2379,http://192.168.60.73:4001 \
--listen-client-urls=http://0.0.0.0:2379,http://0.0.0.0:4001 \
--initial-advertise-peer-urls=http://192.168.60.73:2380 \
--listen-peer-urls=http://0.0.0.0:2380 \
--initial-cluster-token=9477af68bbee1b9ae037d6fd9e7efefd \
--initial-cluster=etcd0=http://192.168.60.71:2380,etcd1=http://192.168.60.72:2380,etcd2=http://192.168.60.73:2380 \
--initial-cluster-state=new \
--auto-tls \
--peer-auto-tls \
--data-dir=/var/lib/etcd
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä¸Šæ£€æŸ¥etcdå¯åŠ¨çŠ¶æ€

```
$ docker exec -ti etcd ash

$ etcdctl member list
1a32c2d3f1abcad0: name=etcd2 peerURLs=http://192.168.60.73:2380 clientURLs=http://192.168.60.73:2379,http://192.168.60.73:4001 isLeader=false
1da4f4e8b839cb79: name=etcd1 peerURLs=http://192.168.60.72:2380 clientURLs=http://192.168.60.72:2379,http://192.168.60.72:4001 isLeader=false
4238bcb92d7f2617: name=etcd0 peerURLs=http://192.168.60.71:2380 clientURLs=http://192.168.60.71:2379,http://192.168.60.71:4001 isLeader=true

$ etcdctl cluster-health
member 1a32c2d3f1abcad0 is healthy: got healthy result from http://192.168.60.73:2379
member 1da4f4e8b839cb79 is healthy: got healthy result from http://192.168.60.72:2379
member 4238bcb92d7f2617 is healthy: got healthy result from http://192.168.60.71:2379
cluster is healthy

$ exit
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### kubeadmåˆå§‹åŒ–

* åœ¨k8s-master1ä¸Šä¿®æ”¹kubeadm-init-v1.7.x.yamlæ–‡ä»¶ï¼Œè®¾ç½®etcd.endpointsçš„${HOST_IP}ä¸ºk8s-master1ã€k8s-master2ã€k8s-master3çš„IPåœ°å€ã€‚è®¾ç½®apiServerCertSANsçš„${HOST_IP}ä¸ºk8s-master1ã€k8s-master2ã€k8s-master3çš„IPåœ°å€ï¼Œ${HOST_NAME}ä¸ºk8s-master1ã€k8s-master2ã€k8s-master3ï¼Œ${VIRTUAL_IP}ä¸ºkeepalivedçš„è™šæ‹ŸIPåœ°å€

```
$ vi /root/kubeadm-ha/kubeadm-init-v1.7.x.yaml 
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
kubernetesVersion: v1.7.0
networking:
  podSubnet: 10.244.0.0/16
apiServerCertSANs:
- k8s-master1
- k8s-master2
- k8s-master3
- 192.168.60.71
- 192.168.60.72
- 192.168.60.73
- 192.168.60.80
etcd:
  endpoints:
  - http://192.168.60.71:2379
  - http://192.168.60.72:2379
  - http://192.168.60.73:2379
```

* å¦‚æœä½¿ç”¨kubeadmåˆå§‹åŒ–é›†ç¾¤ï¼Œå¯åŠ¨è¿‡ç¨‹å¯èƒ½ä¼šå¡åœ¨ä»¥ä¸‹ä½ç½®ï¼Œé‚£ä¹ˆå¯èƒ½æ˜¯å› ä¸ºcgroup-driverå‚æ•°ä¸dockerçš„ä¸ä¸€è‡´å¼•èµ·
* [apiclient] Created API client, waiting for the control plane to become ready
* journalctl -t kubelet -S '2017-06-08'æŸ¥çœ‹æ—¥å¿—ï¼Œå‘ç°å¦‚ä¸‹é”™è¯¯
* error: failed to run Kubelet: failed to create kubelet: misconfiguration: kubelet cgroup driver: "systemd"
* éœ€è¦ä¿®æ”¹KUBELET_CGROUP_ARGS=--cgroup-driver=systemdä¸ºKUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs

```
$ vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
#Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd"
Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=cgroupfs"

$ systemctl daemon-reload && systemctl restart kubelet
```

* åœ¨k8s-master1ä¸Šä½¿ç”¨kubeadmåˆå§‹åŒ–kubernetesé›†ç¾¤ï¼Œè¿æ¥å¤–éƒ¨etcdé›†ç¾¤

```
$ kubeadm init --config=/root/kubeadm-ha/kubeadm-init-v1.7.x.yaml
```

* åœ¨k8s-master1ä¸Šä¿®æ”¹kube-apiserver.yamlçš„admission-controlï¼Œv1.7.0ä½¿ç”¨äº†NodeRestrictionç­‰å®‰å…¨æ£€æŸ¥æ§åˆ¶ï¼ŒåŠ¡å¿…è®¾ç½®æˆv1.6.xæ¨èçš„admission-controlé…ç½®

```
$ vi /etc/kubernetes/manifests/kube-apiserver.yaml
#    - --admission-control=Initializers,NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota
    - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds
```

* åœ¨k8s-master1ä¸Šé‡å¯docker kubeletæœåŠ¡

```
$ systemctl restart docker kubelet
```

* åœ¨k8s-master1ä¸Šè®¾ç½®kubectlçš„ç¯å¢ƒå˜é‡KUBECONFIGï¼Œè¿æ¥kubelet

```
$ vi ~/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf

$ source ~/.bashrc
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### flannelç½‘ç»œç»„ä»¶å®‰è£…

* åœ¨k8s-master1ä¸Šå®‰è£…flannel podç½‘ç»œç»„ä»¶ï¼Œå¿…é¡»å®‰è£…ç½‘ç»œç»„ä»¶ï¼Œå¦åˆ™kube-dns podä¼šä¸€ç›´å¤„äºContainerCreating

```
$ kubectl create -f /root/kubeadm-ha/kube-flannel
clusterrole "flannel" created
clusterrolebinding "flannel" created
serviceaccount "flannel" created
configmap "kube-flannel-cfg" created
daemonset "kube-flannel-ds" created
```

* åœ¨k8s-master1ä¸ŠéªŒè¯kube-dnsæˆåŠŸå¯åŠ¨ï¼Œå¤§æ¦‚ç­‰å¾…3åˆ†é’Ÿï¼ŒéªŒè¯æ‰€æœ‰podsçš„çŠ¶æ€ä¸ºRunning

```
$ kubectl get pods --all-namespaces -o wide
NAMESPACE     NAME                                 READY     STATUS    RESTARTS   AGE       IP              NODE
kube-system   kube-apiserver-k8s-master1           1/1       Running   0          3m        192.168.60.71   k8s-master1
kube-system   kube-controller-manager-k8s-master1  1/1       Running   0          3m        192.168.60.71   k8s-master1
kube-system   kube-dns-3913472980-k9mt6            3/3       Running   0          4m        10.244.0.104    k8s-master1
kube-system   kube-flannel-ds-3hhjd                2/2       Running   0          1m        192.168.60.71   k8s-master1
kube-system   kube-proxy-rzq3t                     1/1       Running   0          4m        192.168.60.71   k8s-master1
kube-system   kube-scheduler-k8s-master1           1/1       Running   0          3m        192.168.60.71   k8s-master1
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### dashboardç»„ä»¶å®‰è£…

* åœ¨k8s-master1ä¸Šå®‰è£…dashboardç»„ä»¶

```
$ kubectl create -f /root/kubeadm-ha/kube-dashboard/
serviceaccount "kubernetes-dashboard" created
clusterrolebinding "kubernetes-dashboard" created
deployment "kubernetes-dashboard" created
service "kubernetes-dashboard" created
```

* åœ¨k8s-master1ä¸Šå¯åŠ¨proxyï¼Œæ˜ å°„åœ°å€åˆ°0.0.0.0

```
$ kubectl proxy --address='0.0.0.0' &
```

* åœ¨æœ¬æœºMacOSXä¸Šè®¿é—®dashboardåœ°å€ï¼ŒéªŒè¯dashboardæˆåŠŸå¯åŠ¨

```
http://k8s-master1:30000
```

![dashboard](images/dashboard.png)

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### heapsterç»„ä»¶å®‰è£…

* åœ¨k8s-master1ä¸Šå…è®¸åœ¨masterä¸Šéƒ¨ç½²podï¼Œå¦åˆ™heapsterä¼šæ— æ³•éƒ¨ç½²

```
$ kubectl taint nodes --all node-role.kubernetes.io/master-
node "k8s-master1" tainted
```

* åœ¨k8s-master1ä¸Šå®‰è£…heapsterç»„ä»¶ï¼Œç›‘æ§æ€§èƒ½

```
$ kubectl create -f /root/kubeadm-ha/kube-heapster
```

* åœ¨k8s-master1ä¸Šé‡å¯dockerä»¥åŠkubeletæœåŠ¡ï¼Œè®©heapsteråœ¨dashboardä¸Šç”Ÿæ•ˆæ˜¾ç¤º

```
$ systemctl restart docker kubelet
```

* åœ¨k8s-masterä¸Šæ£€æŸ¥podsçŠ¶æ€

```
$ kubectl get all --all-namespaces -o wide
NAMESPACE     NAME                                    READY     STATUS    RESTARTS   AGE       IP              NODE
kube-system   heapster-783524908-kn6jd                1/1       Running   1          9m        10.244.0.111    k8s-master1
kube-system   kube-apiserver-k8s-master1              1/1       Running   1          15m       192.168.60.71   k8s-master1
kube-system   kube-controller-manager-k8s-master1     1/1       Running   1          15m       192.168.60.71   k8s-master1
kube-system   kube-dns-3913472980-k9mt6               3/3       Running   3          16m       10.244.0.110    k8s-master1
kube-system   kube-flannel-ds-3hhjd                   2/2       Running   3          13m       192.168.60.71   k8s-master1
kube-system   kube-proxy-rzq3t                        1/1       Running   1          16m       192.168.60.71   k8s-master1
kube-system   kube-scheduler-k8s-master1              1/1       Running   1          15m       192.168.60.71   k8s-master1
kube-system   kubernetes-dashboard-2039414953-d46vw   1/1       Running   1          11m       10.244.0.109    k8s-master1
kube-system   monitoring-grafana-3975459543-8l94z     1/1       Running   1          9m        10.244.0.112    k8s-master1
kube-system   monitoring-influxdb-3480804314-72ltf    1/1       Running   1          9m        10.244.0.113    k8s-master1
```

* åœ¨æœ¬æœºMacOSXä¸Šè®¿é—®dashboardåœ°å€ï¼ŒéªŒè¯heapsteræˆåŠŸå¯åŠ¨ï¼ŒæŸ¥çœ‹Podsçš„CPUä»¥åŠMemoryä¿¡æ¯æ˜¯å¦æ­£å¸¸å‘ˆç°

```
http://k8s-master1:30000
```

![heapster](images/heapster.png)

* è‡³æ­¤ï¼Œç¬¬ä¸€å°masteræˆåŠŸå®‰è£…ï¼Œå¹¶å·²ç»å®Œæˆflannelã€dashboardã€heapsterçš„éƒ¨ç½²

---
[è¿”å›ç›®å½•](#ç›®å½•)

### masteré›†ç¾¤é«˜å¯ç”¨è®¾ç½®

#### å¤åˆ¶é…ç½®

* åœ¨k8s-master1ä¸ŠæŠŠ/etc/kubernetes/å¤åˆ¶åˆ°k8s-master2ã€k8s-master3

```
scp -r /etc/kubernetes/ k8s-master2:/etc/
scp -r /etc/kubernetes/ k8s-master3:/etc/
```

* åœ¨k8s-master2ã€k8s-master3ä¸Šé‡å¯kubeletæœåŠ¡ï¼Œå¹¶æ£€æŸ¥kubeletæœåŠ¡çŠ¶æ€ä¸ºactive (running)

```
$ systemctl daemon-reload && systemctl restart kubelet

$ systemctl status kubelet
â— kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           â””â”€10-kubeadm.conf
   Active: active (running) since Tue 2017-06-27 16:24:22 CST; 1 day 17h ago
     Docs: http://kubernetes.io/docs/
 Main PID: 2780 (kubelet)
   Memory: 92.9M
   CGroup: /system.slice/kubelet.service
           â”œâ”€2780 /usr/bin/kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --require-...
           â””â”€2811 journalctl -k -f
```

* åœ¨k8s-master2ã€k8s-master3ä¸Šè®¾ç½®kubectlçš„ç¯å¢ƒå˜é‡KUBECONFIGï¼Œè¿æ¥kubelet

```
$ vi ~/.bashrc
export KUBECONFIG=/etc/kubernetes/admin.conf

$ source ~/.bashrc
```

* åœ¨k8s-master2ã€k8s-master3æ£€æµ‹èŠ‚ç‚¹çŠ¶æ€ï¼Œå‘ç°èŠ‚ç‚¹å·²ç»åŠ è¿›æ¥

```
$ kubectl get nodes -o wide
NAME          STATUS    AGE       VERSION   EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION
k8s-master1   Ready     26m       v1.7.0    <none>        CentOS Linux 7 (Core)   3.10.0-514.6.1.el7.x86_64
k8s-master2   Ready     2m        v1.7.0    <none>        CentOS Linux 7 (Core)   3.10.0-514.21.1.el7.x86_64
k8s-master3   Ready     2m        v1.7.0    <none>        CentOS Linux 7 (Core)   3.10.0-514.21.1.el7.x86_64
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### ä¿®æ”¹é…ç½®

* åœ¨k8s-master2ã€k8s-master3ä¸Šä¿®æ”¹kube-apiserver.yamlçš„é…ç½®ï¼Œ${HOST_IP}æ”¹ä¸ºæœ¬æœºIP

```
$ vi /etc/kubernetes/manifests/kube-apiserver.yaml
    - --advertise-address=${HOST_IP}
```

* åœ¨k8s-master2å’Œk8s-master3ä¸Šçš„ä¿®æ”¹kubelet.confè®¾ç½®ï¼Œ${HOST_IP}æ”¹ä¸ºæœ¬æœºIP

```
$ vi /etc/kubernetes/kubelet.conf
server: https://${HOST_IP}:6443
```

* åœ¨k8s-master2å’Œk8s-master3ä¸Šä¿®æ”¹admin.confï¼Œ${HOST_IP}ä¿®æ”¹ä¸ºæœ¬æœºIPåœ°å€

```
$ vi /etc/kubernetes/admin.conf
    server: https://${HOST_IP}:6443
```

* åœ¨k8s-master2å’Œk8s-master3ä¸Šä¿®æ”¹controller-manager.confï¼Œ${HOST_IP}ä¿®æ”¹ä¸ºæœ¬æœºIPåœ°å€

```
$ vi /etc/kubernetes/controller-manager.conf
    server: https://${HOST_IP}:6443
```

* åœ¨k8s-master2å’Œk8s-master3ä¸Šä¿®æ”¹scheduler.confï¼Œ${HOST_IP}ä¿®æ”¹ä¸ºæœ¬æœºIPåœ°å€

```
$ vi /etc/kubernetes/scheduler.conf
    server: https://${HOST_IP}:6443
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä¸Šé‡å¯æ‰€æœ‰æœåŠ¡

```
$ systemctl daemon-reload && systemctl restart docker kubelet
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### éªŒè¯é«˜å¯ç”¨å®‰è£…

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä»»æ„èŠ‚ç‚¹ä¸Šæ£€æµ‹æœåŠ¡å¯åŠ¨æƒ…å†µï¼Œå‘ç°apiserverã€controller-managerã€kube-schedulerã€proxyã€flannelå·²ç»åœ¨k8s-master1ã€k8s-master2ã€k8s-master3æˆåŠŸå¯åŠ¨

```
$ kubectl get pod --all-namespaces -o wide | grep k8s-master2
kube-system   kube-apiserver-k8s-master2              1/1       Running   1          55s       192.168.60.72   k8s-master2
kube-system   kube-controller-manager-k8s-master2     1/1       Running   2          18m       192.168.60.72   k8s-master2
kube-system   kube-flannel-ds-t8gkh                   2/2       Running   4          18m       192.168.60.72   k8s-master2
kube-system   kube-proxy-bpgqw                        1/1       Running   1          18m       192.168.60.72   k8s-master2
kube-system   kube-scheduler-k8s-master2              1/1       Running   2          18m       192.168.60.72   k8s-master2

$ kubectl get pod --all-namespaces -o wide | grep k8s-master3
kube-system   kube-apiserver-k8s-master3              1/1       Running   1          1m        192.168.60.73   k8s-master3
kube-system   kube-controller-manager-k8s-master3     1/1       Running   2          18m       192.168.60.73   k8s-master3
kube-system   kube-flannel-ds-tmqmx                   2/2       Running   4          18m       192.168.60.73   k8s-master3
kube-system   kube-proxy-4stg3                        1/1       Running   1          18m       192.168.60.73   k8s-master3
kube-system   kube-scheduler-k8s-master3              1/1       Running   2          18m       192.168.60.73   k8s-master3
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä»»æ„èŠ‚ç‚¹ä¸Šé€šè¿‡kubectl logsæ£€æŸ¥å„ä¸ªcontroller-managerå’Œschedulerçš„leader electionç»“æœï¼Œå¯ä»¥å‘ç°åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹æœ‰æ•ˆè¡¨ç¤ºé€‰ä¸¾æ­£å¸¸

```
$ kubectl logs -n kube-system kube-controller-manager-k8s-master1
$ kubectl logs -n kube-system kube-controller-manager-k8s-master2
$ kubectl logs -n kube-system kube-controller-manager-k8s-master3

$ kubectl logs -n kube-system kube-scheduler-k8s-master1
$ kubectl logs -n kube-system kube-scheduler-k8s-master2
$ kubectl logs -n kube-system kube-scheduler-k8s-master3
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä»»æ„èŠ‚ç‚¹ä¸ŠæŸ¥çœ‹deploymentçš„æƒ…å†µ

```
$ kubectl get deploy --all-namespaces
NAMESPACE     NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kube-system   heapster               1         1         1            1           41m
kube-system   kube-dns               1         1         1            1           48m
kube-system   kubernetes-dashboard   1         1         1            1           43m
kube-system   monitoring-grafana     1         1         1            1           41m
kube-system   monitoring-influxdb    1         1         1            1           41m
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä»»æ„èŠ‚ç‚¹ä¸ŠæŠŠkubernetes-dashboardã€kube-dnsã€ scale upæˆreplicas=3ï¼Œä¿è¯å„ä¸ªmasterèŠ‚ç‚¹ä¸Šéƒ½æœ‰è¿è¡Œ

```
$ kubectl scale --replicas=3 -n kube-system deployment/kube-dns
$ kubectl get pods --all-namespaces -o wide| grep kube-dns

$ kubectl scale --replicas=3 -n kube-system deployment/kubernetes-dashboard
$ kubectl get pods --all-namespaces -o wide| grep kubernetes-dashboard

$ kubectl scale --replicas=3 -n kube-system deployment/heapster
$ kubectl get pods --all-namespaces -o wide| grep heapster

$ kubectl scale --replicas=3 -n kube-system deployment/monitoring-grafana
$ kubectl get pods --all-namespaces -o wide| grep monitoring-grafana

$ kubectl scale --replicas=3 -n kube-system deployment/monitoring-influxdb
$ kubectl get pods --all-namespaces -o wide| grep monitoring-influxdb
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### keepalivedå®‰è£…é…ç½®

* åœ¨k8s-masterã€k8s-master2ã€k8s-master3ä¸Šå®‰è£…keepalived

```
$ yum install -y keepalived

$ systemctl enable keepalived && systemctl restart keepalived
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä¸Šå¤‡ä»½keepalivedé…ç½®æ–‡ä»¶

```
$ mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf.bak
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä¸Šè®¾ç½®apiserverç›‘æ§è„šæœ¬ï¼Œå½“apiserveræ£€æµ‹å¤±è´¥çš„æ—¶å€™å…³é—­keepalivedæœåŠ¡ï¼Œè½¬ç§»è™šæ‹ŸIPåœ°å€

```
$ vi /etc/keepalived/check_apiserver.sh
#!/bin/bash
err=0
for k in $( seq 1 10 )
do
    check_code=$(ps -ef|grep kube-apiserver | wc -l)
    if [ "$check_code" = "1" ]; then
        err=$(expr $err + 1)
        sleep 5
        continue
    else
        err=0
        break
    fi
done
if [ "$err" != "0" ]; then
    echo "systemctl stop keepalived"
    /usr/bin/systemctl stop keepalived
    exit 1
else
    exit 0
fi

chmod a+x /etc/keepalived/check_apiserver.sh
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä¸ŠæŸ¥çœ‹æ¥å£åå­—

```
$ ip a | grep 192.168.60
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä¸Šè®¾ç½®keepalivedï¼Œå‚æ•°è¯´æ˜å¦‚ä¸‹ï¼š
* state ${STATE}ï¼šä¸ºMASTERæˆ–è€…BACKUPï¼Œåªèƒ½æœ‰ä¸€ä¸ªMASTER
* interface ${INTERFACE_NAME}ï¼šä¸ºæœ¬æœºçš„éœ€è¦ç»‘å®šçš„æ¥å£åå­—ï¼ˆé€šè¿‡ä¸Šè¾¹çš„```ip a```å‘½ä»¤æŸ¥çœ‹ï¼‰
* mcast_src_ip ${HOST_IP}ï¼šä¸ºæœ¬æœºçš„IPåœ°å€
* priority ${PRIORITY}ï¼šä¸ºä¼˜å…ˆçº§ï¼Œä¾‹å¦‚102ã€101ã€100ï¼Œä¼˜å…ˆçº§è¶Šé«˜è¶Šå®¹æ˜“é€‰æ‹©ä¸ºMASTERï¼Œä¼˜å…ˆçº§ä¸èƒ½ä¸€æ ·
* ${VIRTUAL_IP}ï¼šä¸ºè™šæ‹Ÿçš„IPåœ°å€ï¼Œè¿™é‡Œè®¾ç½®ä¸º192.168.60.80

```
$ vi /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
}
vrrp_script chk_apiserver {
    script "/etc/keepalived/check_apiserver.sh"
    interval 2
    weight -5
    fall 3  
    rise 2
}
vrrp_instance VI_1 {
    state ${STATE}
    interface ${INTERFACE_NAME}
    mcast_src_ip ${HOST_IP}
    virtual_router_id 51
    priority ${PRIORITY}
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass 4be37dc3b4c90194d1600c483e10ad1d
    }
    virtual_ipaddress {
        ${VIRTUAL_IP}
    }
    track_script {
       chk_apiserver
    }
}
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä¸Šé‡å¯keepalivedæœåŠ¡ï¼Œæ£€æµ‹è™šæ‹ŸIPåœ°å€æ˜¯å¦ç”Ÿæ•ˆ

```
$ systemctl restart keepalived
$ ping 192.168.60.80
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### nginxè´Ÿè½½å‡è¡¡é…ç½®

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä¸Šä¿®æ”¹nginx-default.confè®¾ç½®ï¼Œ${HOST_IP}å¯¹åº”k8s-master1ã€k8s-master2ã€k8s-master3çš„åœ°å€ã€‚é€šè¿‡nginxæŠŠè®¿é—®apiserverçš„6443ç«¯å£è´Ÿè½½å‡è¡¡åˆ°8433ç«¯å£ä¸Š

```
$ vi /root/kubeadm-ha/nginx-default.conf
stream {
    upstream apiserver {
        server ${HOST_IP}:6443 weight=5 max_fails=3 fail_timeout=30s;
        server ${HOST_IP}:6443 weight=5 max_fails=3 fail_timeout=30s;
        server ${HOST_IP}:6443 weight=5 max_fails=3 fail_timeout=30s;
    }

    server {
        listen 8443;
        proxy_connect_timeout 1s;
        proxy_timeout 3s;
        proxy_pass apiserver;
    }
}
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä¸Šå¯åŠ¨nginxå®¹å™¨

```
$ docker run -d -p 8443:8443 \
--name nginx-lb \
--restart always \
-v /root/kubeadm-ha/nginx-default.conf:/etc/nginx/nginx.conf \
nginx
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä¸Šæ£€æµ‹keepalivedæœåŠ¡çš„è™šæ‹ŸIPåœ°å€æŒ‡å‘

```
$ curl -L 192.168.60.80:8443 | wc -l
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100    14    0    14    0     0  18324      0 --:--:-- --:--:-- --:--:-- 14000
1
```

* ä¸šåŠ¡æ¢å¤ååŠ¡å¿…é‡å¯keepalivedï¼Œå¦åˆ™keepalivedä¼šå¤„äºå…³é—­çŠ¶æ€

```
$ systemctl restart keepalived
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä¸ŠæŸ¥çœ‹keeplivedæ—¥å¿—ï¼Œæœ‰ä»¥ä¸‹è¾“å‡ºè¡¨ç¤ºå½“å‰è™šæ‹ŸIPåœ°å€ç»‘å®šçš„ä¸»æœº

```
$ systemctl status keepalived -l
VRRP_Instance(VI_1) Sending gratuitous ARPs on ens160 for 192.168.60.80
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### kube-proxyé…ç½®

* åœ¨k8s-master1ä¸Šè®¾ç½®kube-proxyä½¿ç”¨keepalivedçš„è™šæ‹ŸIPåœ°å€ï¼Œé¿å…k8s-master1å¼‚å¸¸çš„æ—¶å€™æ‰€æœ‰èŠ‚ç‚¹çš„kube-proxyè¿æ¥ä¸ä¸Š

```
$ kubectl get -n kube-system configmap
NAME                                 DATA      AGE
extension-apiserver-authentication   6         4h
kube-flannel-cfg                     2         4h
kube-proxy                           1         4h
```

* åœ¨k8s-master1ä¸Šä¿®æ”¹configmap/kube-proxyçš„serveræŒ‡å‘keepalivedçš„è™šæ‹ŸIPåœ°å€

```
$ kubectl edit -n kube-system configmap/kube-proxy
        server: https://192.168.60.80:8443
```

* åœ¨k8s-master1ä¸ŠæŸ¥çœ‹configmap/kube-proxyè®¾ç½®æƒ…å†µ

```
$ kubectl get -n kube-system configmap/kube-proxy -o yaml
```

* åœ¨k8s-master1ä¸Šåˆ é™¤æ‰€æœ‰kube-proxyçš„podï¼Œè®©proxyé‡å»º

```
kubectl get pods --all-namespaces -o wide | grep proxy
```

* åœ¨k8s-master1ã€k8s-master2ã€k8s-master3ä¸Šé‡å¯docker kubelet keepalivedæœåŠ¡

```
$ systemctl restart docker kubelet keepalived
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### éªŒè¯masteré›†ç¾¤é«˜å¯ç”¨

* åœ¨k8s-master1ä¸Šæ£€æŸ¥å„ä¸ªèŠ‚ç‚¹podçš„å¯åŠ¨çŠ¶æ€ï¼Œæ¯ä¸ªä¸Šéƒ½æˆåŠŸå¯åŠ¨heapsterã€kube-apiserverã€kube-controller-managerã€kube-dnsã€kube-flannelã€kube-proxyã€kube-schedulerã€kubernetes-dashboardã€monitoring-grafanaã€monitoring-influxdbã€‚å¹¶ä¸”æ‰€æœ‰podéƒ½å¤„äºRunningçŠ¶æ€è¡¨ç¤ºæ­£å¸¸

```
$ kubectl get pods --all-namespaces -o wide | grep k8s-master1

$ kubectl get pods --all-namespaces -o wide | grep k8s-master2

$ kubectl get pods --all-namespaces -o wide | grep k8s-master3
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

### nodeèŠ‚ç‚¹åŠ å…¥é«˜å¯ç”¨é›†ç¾¤è®¾ç½®

#### kubeadmåŠ å…¥é«˜å¯ç”¨é›†ç¾¤
* åœ¨k8s-master1ä¸Šç¦æ­¢åœ¨æ‰€æœ‰masterèŠ‚ç‚¹ä¸Šå‘å¸ƒåº”ç”¨

```
$ kubectl patch node k8s-master1 -p '{"spec":{"unschedulable":true}}'

$ kubectl patch node k8s-master2 -p '{"spec":{"unschedulable":true}}'

$ kubectl patch node k8s-master3 -p '{"spec":{"unschedulable":true}}'
```

* åœ¨k8s-master1ä¸ŠæŸ¥çœ‹é›†ç¾¤çš„token

```
$ kubeadm token list
TOKEN           TTL         EXPIRES   USAGES                   DESCRIPTION
xxxxxx.yyyyyy   <forever>   <never>   authentication,signing   The default bootstrap token generated by 'kubeadm init'
```

* åœ¨k8s-node1 ~ k8s-node8ä¸Šï¼Œ${TOKEN}ä¸ºk8s-master1ä¸Šæ˜¾ç¤ºçš„tokenï¼Œ${VIRTUAL_IP}ä¸ºkeepalivedçš„è™šæ‹ŸIPåœ°å€192.168.60.80

```
$ kubeadm join --token ${TOKEN} ${VIRTUAL_IP}:8443
```

---
[è¿”å›ç›®å½•](#ç›®å½•)

#### éƒ¨ç½²åº”ç”¨éªŒè¯é›†ç¾¤

* åœ¨k8s-node1 ~ k8s-node8ä¸ŠæŸ¥çœ‹kubeletçŠ¶æ€ï¼ŒkubeletçŠ¶æ€ä¸ºactive (running)è¡¨ç¤ºkubeletæœåŠ¡æ­£å¸¸å¯åŠ¨

```
$ systemctl status kubelet
â— kubelet.service - kubelet: The Kubernetes Node Agent
   Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled)
  Drop-In: /etc/systemd/system/kubelet.service.d
           â””â”€10-kubeadm.conf
   Active: active (running) since Tue 2017-06-27 16:23:43 CST; 1 day 18h ago
     Docs: http://kubernetes.io/docs/
 Main PID: 1146 (kubelet)
   Memory: 204.9M
   CGroup: /system.slice/kubelet.service
           â”œâ”€ 1146 /usr/bin/kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --require...
           â”œâ”€ 2553 journalctl -k -f
           â”œâ”€ 4988 /usr/sbin/glusterfs --log-level=ERROR --log-file=/var/lib/kubelet/pl...
           â””â”€14720 /usr/sbin/glusterfs --log-level=ERROR --log-file=/var/lib/kubelet/pl...
```

* åœ¨k8s-master1ä¸Šæ£€æŸ¥å„ä¸ªèŠ‚ç‚¹çŠ¶æ€ï¼Œå‘ç°æ‰€æœ‰k8s-nodesèŠ‚ç‚¹æˆåŠŸåŠ å…¥

```
$ kubectl get nodes -o wide
NAME          STATUS                     AGE       VERSION
k8s-master1   Ready,SchedulingDisabled   5h        v1.7.0
k8s-master2   Ready,SchedulingDisabled   4h        v1.7.0
k8s-master3   Ready,SchedulingDisabled   4h        v1.7.0
k8s-node1     Ready                      6m        v1.7.0
k8s-node2     Ready                      4m        v1.7.0
k8s-node3     Ready                      4m        v1.7.0
k8s-node4     Ready                      3m        v1.7.0
k8s-node5     Ready                      3m        v1.7.0
k8s-node6     Ready                      3m        v1.7.0
k8s-node7     Ready                      3m        v1.7.0
k8s-node8     Ready                      3m        v1.7.0
```

* åœ¨k8s-master1ä¸Šæµ‹è¯•éƒ¨ç½²nginxæœåŠ¡ï¼ŒnginxæœåŠ¡æˆåŠŸéƒ¨ç½²åˆ°k8s-node5ä¸Š

```
$ kubectl run nginx --image=nginx --port=80
deployment "nginx" created

$ kubectl get pod -o wide -l=run=nginx
NAME                     READY     STATUS    RESTARTS   AGE       IP           NODE
nginx-2662403697-pbmwt   1/1       Running   0          5m        10.244.7.6   k8s-node5
```

* åœ¨k8s-master1è®©nginxæœåŠ¡å¤–éƒ¨å¯è§

```
$ kubectl expose deployment nginx --port=80 --target-port=80 --type=NodePort
service "nginx" exposed

$ kubectl get svc -l=run=nginx
NAME      CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
nginx     10.105.151.69   <nodes>       80:31639/TCP   43s

$ curl k8s-master2:31639
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>

```

* è‡³æ­¤ï¼Œkubernetesé«˜å¯ç”¨é›†ç¾¤æˆåŠŸéƒ¨ç½² ğŸ˜€
---
[è¿”å›ç›®å½•](#ç›®å½•)

